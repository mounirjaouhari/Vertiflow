# Documentations sur les algorithmes 

# üìò ALGORITHME A1 : NORMALISATION JSON & STANDARDISATION

Ticket JIRA : TICKET-019

Responsable : @Mouhammed (Data Engineer)

Sprint : Semaine 2 - Phase ETL

## 1. Emplacement dans l'Architecture

Cet algorithme se situe au tout d√©but de la cha√Æne de traitement, dans la **Zone 1 (Collection & Normalisation)** de **Apache NiFi**.

- **Entr√©e :** Donn√©es h√©t√©rog√®nes (Binaires MQTT, CSV Labo, JSON imbriqu√© API).
- **Sortie :** JSON standardis√© "√† plat" respectant le sch√©ma des 153 colonnes.
- **Composant NiFi :** `JoltTransformJSON` ou `ExecuteScript` (Python).

## 2. Description Scientifique & Technique

L'objectif est de transformer des donn√©es "sales" et disparates en un format unique compr√©hensible par le reste du syst√®me (Kafka, ClickHouse, Mongo).

### Probl√®me : L'H√©t√©rog√©n√©it√© des Sources

- **Capteur A (MQTT) envoie :** `{"v": 24.5, "id": "S1"}` (Noms courts pour √©conomiser la batterie).
- **Capteur B (API) envoie :** `{"temperature": {"value": 24.5, "unit": "C"}, "device": "S1"}` (Structure imbriqu√©e).
- **Labo (CSV) envoie :** `24.5;S1;2025-12-31` (Pas de cl√©s).

### Solution : Le Mapping Pivot

L'algorithme applique une transformation pour aligner toutes ces variantes sur le sch√©ma canonique `telemetry_v3.json` :

- `v` -> `air_temp_internal`
- `id` -> `sensor_hardware_id`
- Conversion de types (String "24.5" -> Float 24.5).

## 3. Impl√©mentation (Sp√©cification JOLT pour NiFi)

Si vous utilisez le processeur standard `JoltTransformJSON`, voici la sp√©cification "Shift" pour normaliser un message MQTT typique.

```
[
  {
    "operation": "shift",
    "spec": {
      "t": "timestamp",
      "id": "sensor_hardware_id",
      "val": {
        "temp": "air_temp_internal",
        "hum": "air_humidity",
        "co2": "co2_level_ambient",
        "ppfd": "light_intensity_ppfd",
        "ec": "nutrient_solution_ec",
        "ph": "water_ph"
      },
      "meta": {
        "bat": "ups_battery_health",
        "err": "sensor_calibration_offset"
      }
    }
  },
  {
    "operation": "default",
    "spec": {
      "data_source_type": "IoT",
      "data_integrity_flag": 0
    }
  }
]
```

## 4. Impl√©mentation Alternative (Script Python)

Pour des transformations plus complexes (nettoyage de cha√Ænes, calculs simples), on utilise `ExecuteScript` avec Python.

```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ALGORITHME A1 : NORMALISATION UNIVERSELLE
================================================================================
Description :
Convertit les payloads entrants en dictionnaire Python standardis√©
correspondant aux 153 colonnes du Golden Record.
G√®re les conversions de types et les valeurs par d√©faut.
================================================================================
"""

import json
import java.io
from org.apache.commons.io import IOUtils
from java.nio.charset import StandardCharsets
from org.apache.nifi.processor.io import StreamCallback

class Standardizer(StreamCallback):
    def __init__(self):
        pass

    def process(self, inputStream, outputStream):
        # 1. Lecture de l'entr√©e brute
        text = IOUtils.toString(inputStream, StandardCharsets.UTF_8)
        try:
            raw_data = json.loads(text)
        except ValueError:
            # Si ce n'est pas du JSON, on rejette (sera g√©r√© par le Failure)
            raise

        # 2. Cr√©ation du Golden Record vide (Structure cible)
        golden_record = {
            "timestamp": None,
            "sensor_hardware_id": None,
            "air_temp_internal": None,
            "air_humidity": None,
            # ... (autres colonnes initialis√©es √† None)
            "data_source_type": "IoT_Sensor"
        }

        # 3. Logique de Mapping (Exemple pour un capteur MQTT compact)
        # Mapping explicite : Source -> Cible
        if 't' in raw_data:
            golden_record['timestamp'] = raw_data['t'] # Format ISO8601 attendu
        
        if 'id' in raw_data:
            golden_record['sensor_hardware_id'] = str(raw_data['id']).upper()

        # Extraction des mesures (Aplatissement)
        if 'm' in raw_data: # 'm' pour mesures
            measures = raw_data['m']
            golden_record['air_temp_internal'] = self.safe_float(measures.get('t'))
            golden_record['air_humidity'] = self.safe_float(measures.get('h'))
            golden_record['co2_level_ambient'] = self.safe_int(measures.get('c'))

        # 4. √âcriture de la sortie standardis√©e
        outputStream.write(json.dumps(golden_record).encode('utf-8'))

    def safe_float(self, value):
        """Conversion s√©curis√©e en Float32"""
        try:
            return float(value) if value is not None else None
        except (ValueError, TypeError):
            return None

    def safe_int(self, value):
        """Conversion s√©curis√©e en UInt16"""
        try:
            return int(value) if value is not None else None
        except (ValueError, TypeError):
            return None

# Ex√©cution NiFi
flowFile = session.get()
if flowFile is not None:
    try:
        flowFile = session.write(flowFile, Standardizer())
        session.transfer(flowFile, REL_SUCCESS)
    except Exception as e:
        # En cas d'erreur de parsing JSON critique
        session.transfer(flowFile, REL_FAILURE)
```

## 5. Crit√®res de Validation (Definition of Done)

- [ ] Toutes les cl√©s du JSON de sortie existent dans le sch√©ma `telemetry_v3.json`.
- [ ] Les types de donn√©es sont corrects (Pas de "24.5" string dans un champ float).
- [ ] Les champs manquants sont mis √† `null` (pas d'erreur de cl√© manquante plus loin).
- [ ] Le timestamp est au format ISO 8601 UTC (`2025-12-31T12:00:00.000Z`).

# üõ°Ô∏è ALGORITHME A2 : D√âTECTION D'ABERRATION (Z-SCORE)

**Ticket JIRA :** `TICKET-020` **Responsable :** @Mouhammed (Data Engineer) **Sprint :** Semaine 2 - Phase ETL

## 1. Emplacement dans l'Architecture

Cet algorithme est ex√©cut√© dans la **Zone 2 (Fusion & Enrichissement)** de **Apache NiFi**, juste apr√®s la normalisation et l'enrichissement, mais avant l'envoi vers Kafka.

- **Entr√©e :** JSON standardis√© avec valeurs brutes (ex: `air_temp_internal`).
- **Sortie :** JSON enrichi avec les flags de qualit√© (`data_integrity_flag`, `anomaly_confidence_score`).
- **Composant NiFi :** `ExecuteScript` (Jython - Python sur JVM).

## 2. Description Scientifique & Technique

L'objectif est de filtrer les "bruits" et les erreurs de capteurs (valeurs aberrantes) sans perdre de donn√©es. Une donn√©e n'est jamais supprim√©e, elle est "marqu√©e".

### Probl√®me : La fiabilit√© des Capteurs IoT

Un capteur de temp√©rature peut, √† cause d'une baisse de tension ou d'une interf√©rence, envoyer soudainement une valeur de **85¬∞C** ou **-50¬∞C** dans une serre chauff√©e. Si cette donn√©e entre dans la moyenne scientifique (ClickHouse), elle faussera toutes les corr√©lations (Algo A7).

### Solution : Le Test Statistique Z-Score

Le Z-Score mesure √† combien d'√©carts-types ($\sigma$) une donn√©e se trouve de la moyenne ($\mu$).

$$Z = \frac{X - \mu}{\sigma}$$

- Si $-3 < Z < 3$ : La donn√©e est consid√©r√©e comme **STATISTIQUEMENT NORMALE** (99.7% des cas).
- Si $|Z| \ge 3$ : La donn√©e est une **ANOMALIE** (Outlier).

## 3. Impl√©mentation (Script Python pour NiFi)

Ce script maintient un √©tat l√©ger (moyenne glissante simplifi√©e ou constantes de r√©f√©rence) pour calculer le score. Pour la production, les constantes $\mu$ et $\sigma$ sont souvent charg√©es depuis le *DistributedMapCache* de NiFi.

```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ALGORITHME A2 : Z-SCORE FILTER
================================================================================
Description :
Analyse chaque mesure entrante et calcule son Z-Score par rapport aux
r√©f√©rences historiques de la ferme.
Marque les donn√©es comme 'VALID' (0) ou 'INVALID' (1).
================================================================================
"""

import json
import math
from org.apache.commons.io import IOUtils
from java.nio.charset import StandardCharsets
from org.apache.nifi.processor.io import StreamCallback

class QualityControl(StreamCallback):
    def __init__(self):
        # --- CONFIGURATION (Cibles Expert - √Ä dynamiser via Cache) ---
        # Format: 'colonne': (Moyenne_Attendue, Ecart_Type_Tol√©r√©)
        self.stats_profile = {
            'air_temp_internal': (24.0, 3.5),      # Moy 24¬∞C, Varie entre 13.5 et 34.5
            'air_humidity': (60.0, 15.0),          # Moy 60%, Varie bcp
            'nutrient_solution_ec': (1.8, 0.4),    # EC tr√®s stable
            'water_ph': (6.0, 0.5)                 # pH critique
        }
        self.SIGMA_THRESHOLD = 3.0

    def process(self, inputStream, outputStream):
        # 1. Lecture
        text = IOUtils.toString(inputStream, StandardCharsets.UTF_8)
        try:
            record = json.loads(text)
        except ValueError:
            # Pas de JSON ? Laissez passer, l'algo A1 aurait d√ª filtrer.
            outputStream.write(text.encode('utf-8'))
            return

        # 2. Analyse
        anomalies_found = 0
        details = []

        # On it√®re sur les champs qu'on sait surveiller
        for field, (mean, std_dev) in self.stats_profile.items():
            val = record.get(field)
            
            if val is not None and isinstance(val, (int, float)):
                # Calcul Z-Score
                z_score = (val - mean) / std_dev
                
                # Enrichissement (Optionnel : stocker le score pour analyse fine)
                # record[f'meta_zscore_{field}'] = round(z_score, 2)

                if abs(z_score) > self.SIGMA_THRESHOLD:
                    anomalies_found += 1
                    details.append(f"{field}:val={val}:z={z_score:.1f}")

        # 3. Marquage (Flagging)
        # 0 = OK, 1 = Warning/Rejet Statistique
        if anomalies_found > 0:
            record['data_integrity_flag'] = 1
            record['anomaly_confidence_score'] = min(1.0, anomalies_found * 0.5) # Score simple
            record['processing_notes'] = f"OUTLIERS_DETECTED: {','.join(details)}"
        else:
            if 'data_integrity_flag' not in record:
                record['data_integrity_flag'] = 0
            record['anomaly_confidence_score'] = 0.0

        # 4. √âcriture
        outputStream.write(json.dumps(record).encode('utf-8'))

# Ex√©cution NiFi Boilerplate
flowFile = session.get()
if flowFile is not None:
    try:
        # Ex√©cute le callback
        flowFile = session.write(flowFile, QualityControl())
        
        # Routage intelligent bas√© sur le r√©sultat
        # On lit l'attribut qu'on vient potentiellement d'√©crire (ou on le d√©duit)
        # Ici on transf√®re tout en SUCCESS, le filtrage se fera par RouteOnAttribute plus tard
        session.transfer(flowFile, REL_SUCCESS)
    except Exception as e:
        session.getLogger().error(f"Erreur Algo A2: {str(e)}")
        session.transfer(flowFile, REL_FAILURE)
```

## 4. Crit√®res de Validation (Definition of Done)

- [ ] Le script ne plante pas si un champ est manquant (`None`).
- [ ] Une temp√©rature de **24¬∞C** donne un Z-Score de 0 et un flag `0`.
- [ ] Une temp√©rature de **80¬∞C** (Z > 10) donne un flag `1` et une note dans `processing_notes`.
- [ ] Le format de sortie reste strictement conforme au sch√©ma JSON global (pas de suppression de champs).

# üîó ALGORITHME A3 : ENRICHISSEMENT CONTEXTUEL & FUSION

**Ticket JIRA :** `TICKET-024` **Responsable :** @Mouhammed (Data Engineer) **Sprint :** Semaine 2 - Phase ETL

## 1. Emplacement dans l'Architecture

Cet algorithme est ex√©cut√© dans la **Zone 2 (Fusion & Enrichissement)** de **Apache NiFi**. Il intervient apr√®s la normalisation (A1) et la validation statistique (A2).

- **Entr√©e :** Donn√©e normalis√©e avec un identifiant technique (ex: `sensor_hardware_id`).
- **Sortie :** Donn√©e enrichie avec le contexte spatial, juridique et environnemental (ex: `parcel_id`, `ext_temp_nasa`).
- **Composant NiFi :** `LookupRecord` + `InvokeHTTP` (Mise en cache).

## 2. Description Scientifique & Technique

Une mesure brute (ex: "24¬∞C sur le capteur S1") n'a aucune valeur pour l'√©tude scientifique si on ne sait pas **o√π** elle a √©t√© prise et **quelles √©taient les conditions externes** √† ce moment pr√©cis.

### Probl√®me : L'Isolation de la Donn√©e

- Le capteur ne conna√Æt pas le bail agricole (`parcel_id`).
- Le capteur ne sait pas qu'il pleut dehors (donn√©e NASA).
- Le capteur ne conna√Æt pas sa position 3D dans le rack (`level_index`).

### Solution : La Jointure Temporelle & Spatiale

L'algorithme A3 r√©alise une **jointure √† la vol√©e** (Lookup) entre le flux temps r√©el et des r√©f√©rentiels statiques ou dynamiques.

1. **R√©f√©rentiel Topologique (Statique) :**
   - Cl√© : `sensor_hardware_id`
   - Valeurs : `farm_id`, `parcel_id`, `rack_id`, `level_index`, `zone_id`.
   - *Pourquoi ?* Pour lier la biologie (plante) au droit (bail).
2. **R√©f√©rentiel Environnemental (Dynamique/Cache) :**
   - Cl√© : `timestamp` (arrondi √† l'heure) + `geo_location`.
   - Valeurs : `ext_temp_nasa`, `ext_solar_radiation`.
   - *Pourquoi ?* Pour calculer l'efficacit√© √©nerg√©tique (Isolation du b√¢timent).

## 3. Impl√©mentation (Script Python pour NiFi `ExecuteScript`)

Bien que `LookupRecord` soit le processeur standard, une impl√©mentation Python offre plus de flexibilit√© pour g√©rer le cache m√©t√©o et les r√®gles m√©tier complexes de fusion.

```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ALGORITHME A3 : CONTEXT ENRICHMENT
================================================================================
Description :
Enrichit le flux de donn√©es avec :
1. La topologie (O√π est le capteur ?) via un dictionnaire en m√©moire.
2. La m√©t√©o externe (Quel temps fait-il ?) via un cache local (NASA API).
================================================================================
"""

import json
from org.apache.commons.io import IOUtils
from java.nio.charset import StandardCharsets
from org.apache.nifi.processor.io import StreamCallback

class ContextEnricher(StreamCallback):
    def __init__(self):
        # 1. CHARGEMENT TOPOLOGIE (Simul√© - En prod, vient d'un DistributedMapCache ou MongoDB)
        # Mapping: Sensor_ID -> { Context Data }
        self.topology_map = {
            "SN-001": {
                "farm_id": "VERT-MAROC-01",
                "parcel_id": "830-AB-123",
                "rack_id": "R01",
                "level_index": 1,
                "zone_id": "ZONE_GERMINATION"
            },
            "SN-002": {
                "farm_id": "VERT-MAROC-01",
                "parcel_id": "830-AB-123",
                "rack_id": "R04",
                "level_index": 5,
                "zone_id": "ZONE_CROISSANCE"
            }
        }
        
        # 2. CACHE M√âT√âO (Simul√© - Mis √† jour par un autre processeur InvokeHTTP)
        self.weather_cache = {
            "current": {
                "ext_temp_nasa": 18.5,
                "ext_humidity_nasa": 45.0,
                "ext_solar_radiation": 850.0
            }
        }

    def process(self, inputStream, outputStream):
        text = IOUtils.toString(inputStream, StandardCharsets.UTF_8)
        try:
            record = json.loads(text)
        except ValueError:
            return # Erreur JSON g√©r√©e en amont

        sensor_id = record.get('sensor_hardware_id')

        # --- A. ENRICHISSEMENT TOPOLOGIQUE ---
        if sensor_id in self.topology_map:
            context = self.topology_map[sensor_id]
            # Fusion des dictionnaires
            record.update(context)
            record['enrichment_status'] = "FULL"
        else:
            # Capteur inconnu (Nouveau ?)
            record['enrichment_status'] = "PARTIAL_UNKNOWN_DEVICE"
            record['farm_id'] = "UNKNOWN" # Valeur par d√©faut pour ne pas casser ClickHouse

        # --- B. ENRICHISSEMENT ENVIRONNEMENTAL ---
        # On ajoute les donn√©es NASA actuelles √† chaque message
        # Cela permet √† l'Algo A7 (Corr√©lation) de comparer Int√©rieur vs Ext√©rieur
        record.update(self.weather_cache['current'])

        # --- C. CALCULS D√âRIV√âS IMM√âDIATS ---
        # Exemple : Delta Temp√©rature (Isolation)
        if record.get('air_temp_internal') is not None:
            internal = float(record['air_temp_internal'])
            external = self.weather_cache['current']['ext_temp_nasa']
            record['temp_delta_isolation'] = round(internal - external, 2)

        outputStream.write(json.dumps(record).encode('utf-8'))

# Ex√©cution NiFi
flowFile = session.get()
if flowFile is not None:
    try:
        flowFile = session.write(flowFile, ContextEnricher())
        session.transfer(flowFile, REL_SUCCESS)
    except Exception as e:
        session.getLogger().error(f"Erreur A3: {str(e)}")
        session.transfer(flowFile, REL_FAILURE)
```

## 4. Crit√®res de Validation (Definition of Done)

- [ ] Chaque ligne sortante poss√®de obligatoirement un `parcel_id` (vital pour la requ√™te SQL de rentabilit√© du bail).
- [ ] Les donn√©es NASA (`ext_temp_nasa`) sont pr√©sentes m√™me si le capteur est en int√©rieur.
- [ ] Si un capteur est inconnu, le flux ne plante pas (valeur par d√©faut "UNKNOWN").
- [ ] Le calcul du delta de temp√©rature est correct (Int√©rieur - Ext√©rieur).

# üõ°Ô∏è ALGORITHME A4 : SEUILLAGE & ALERTE (THRESHOLDING)

**Ticket JIRA :** `TICKET-021` **Responsable :** @Asama (Biologiste) & @Imrane (DevOps) **Sprint :** Semaine 2 - Phase ETL

## 1. Emplacement dans l'Architecture

Cet algorithme est un composant **hybride**.

- **Logique Primaire (Filtrage rapide) :** Ex√©cut√©e dans **Apache NiFi** (Zone 2/3) via `ExecuteScript`.
- **Logique Secondaire (R√©action complexe) :** Ex√©cut√©e dans **MongoDB** (Change Streams) pour la persistance de l'√©tat d'alerte.
- **Entr√©e :** Donn√©e enrichie (A3) contenant les valeurs r√©elles (ex: `nutrient_n_total`).
- **Sortie :** Donn√©e marqu√©e avec un statut d'alerte (`alert_status`, `alert_details`, `maintenance_urgency_score`).

## 2. Description Scientifique & Technique

L'objectif est de prot√©ger l'actif biologique (les plantes) en d√©tectant **imm√©diatement** toute d√©viation par rapport aux conditions optimales de croissance.

### Probl√®me : La Loi de Liebig (Facteur Limitant)

La croissance d'une plante n'est pas limit√©e par le total des ressources disponibles, mais par la ressource la plus rare (le facteur limitant).

- Si Azote (N) est bas, m√™me avec une lumi√®re parfaite, la plante ne grandira pas.
- Une temp√©rature excessive (>30¬∞C) peut tuer la culture en quelques heures.

### Solution : Comparaison Dynamique

L'algorithme compare chaque m√©trique entrante ($Valeur_{Reelle}$) aux cibles d√©finies par l'agronome ($Cible_{Min}, Cible_{Max}$).

$$Etat =  \begin{cases}  NORMAL & \text{si } Cible_{Min} \le Valeur_{Reelle} \le Cible_{Max} \\ ALERTE & \text{sinon} \end{cases}$$

De plus, il calcule un **Score d'Urgence** (`maintenance_urgency_score` - Col 152) proportionnel √† la gravit√© de l'√©cart et √† la dur√©e de l'anomalie.

## 3. Impl√©mentation (Logique Python pour NiFi)

Le script ci-dessous est celui inject√© par l'automatisme de d√©ploiement (`scripts/setup_nifi_algo_a4.py`). Il illustre la logique de d√©cision.

```
# Extrait de la logique m√©tier (ExecuteScript NiFi)

# Cibles (En production, ces valeurs proviennent du Cache/Lookup A3)
TARGETS = {
    'nutrient_n_total': {'min': 100, 'max': 200}, # ppm
    'air_temp_internal': {'min': 18, 'max': 28},  # ¬∞C
    'vapor_pressure_deficit': {'min': 0.4, 'max': 1.5} # kPa
}

def check_thresholds(record):
    alerts = []
    urgency = 0
    
    # 1. Azote (Nutrition)
    n_val = record.get('nutrient_n_total')
    if n_val:
        if n_val < TARGETS['nutrient_n_total']['min']:
            alerts.append(f"CRITIQUE: Carence Azote ({n_val} ppm)")
            urgency += 50 # Haute priorit√©
        elif n_val > TARGETS['nutrient_n_total']['max']:
            alerts.append(f"WARN: Exc√®s Azote ({n_val} ppm)")
            urgency += 20 # Priorit√© moyenne

    # 2. Temp√©rature (Climat)
    t_val = record.get('air_temp_internal')
    if t_val:
         if t_val > TARGETS['air_temp_internal']['max']:
            alerts.append(f"CRITIQUE: Surchauffe ({t_val} ¬∞C)")
            urgency += 80 # Tr√®s haute priorit√© (Risque mortel)

    # R√©sultat
    if alerts:
        record['alert_status'] = "ACTIVE"
        record['alert_details'] = "; ".join(alerts)
        record['maintenance_urgency_score'] = min(100, urgency)
    else:
        record['alert_status'] = "NORMAL"
        record['maintenance_urgency_score'] = 0
        
    return record
```

## 4. Crit√®res de Validation (Definition of Done)

- [ ] Si `nutrient_n_total` = 90 (Cible min 100), le champ `alert_status` passe √† "ACTIVE".
- [ ] Le `maintenance_urgency_score` refl√®te correctement la gravit√© (Surchauffe > Carence l√©g√®re).
- [ ] Les alertes g√©n√©r√©es contiennent un message clair pour l'op√©rateur (`alert_details`).
- [ ] Le flux de donn√©es continue m√™me en cas d'alerte (pas de blocage du pipeline, juste un marquage).

# ‚öôÔ∏è ALGORITHME A5 : MOTEUR DE R√àGLES M√âTIER (RULE ENGINE)

**Ticket JIRA :** `TICKET-025` **Responsable :** @Imrane (DevOps) & @Asama (Biologiste) **Sprint :** Semaine 2 - Phase ETL

## 1. Emplacement dans l'Architecture

Cet algorithme r√©side dans le **Cerveau R√©flexe (Zone 4)**, au niveau de la base de donn√©es **MongoDB**.

- **Entr√©e :** Flux de donn√©es temps r√©el (via Kafka Connector ou Change Stream).
- **Sortie :** Ordres d'action imm√©diats (ex: `STOP_PUMP`, `OPEN_VALVE`) et notifications d'urgence.
- **Composant Technique :** MongoDB Triggers (Atlas) ou Change Streams (Self-Hosted).

## 2. Description Scientifique & Technique

Contrairement √† l'Algorithme A4 (Seuillage) qui surveille des tendances continues (ex: temp√©rature qui monte), l'Algorithme A5 g√®re des **√©tats binaires critiques**.

### Probl√®me : La S√©curit√© Physique

Certains √©v√©nements ne tol√®rent aucune latence ni interpr√©tation.

- Si le capteur d'inondation (`leak_detection_status`) passe √† 1, il faut couper l'eau **tout de suite**.
- Si le bouton d'arr√™t d'urgence (`emergency_stop_status`) est activ√©, tout doit s'arr√™ter.

### Solution : Logique Bool√©enne D√©terministe

L'algorithme applique une s√©rie de r√®gles `IF / THEN` strictes sur chaque document ins√©r√©.

| R√®gle                  | Condition (Si...)                  | Action (Alors...)           | Priorit√©      |
| ---------------------- | ---------------------------------- | --------------------------- | ------------- |
| **R1 (Inondation)**    | `leak_detection_status == 1`       | Couper Pompe Principale     | CRITIQUE (P0) |
| **R2 (Arr√™t Urgence)** | `emergency_stop_status == 1`       | Couper Alimentation 24V     | CRITIQUE (P0) |
| **R3 (Pression)**      | `irrigation_line_pressure > 5 Bar` | Ouvrir Vanne de D√©charge    | HAUTE (P1)    |
| **R4 (Communication)** | `ingestion_lag_ms > 5000`          | Passer en Mode "Safe Local" | MOYENNE (P2)  |

## 3. Impl√©mentation (Script MongoDB Change Stream)

Ce script Node.js (ou Mongo Shell) √©coute la collection `LiveState` et r√©agit instantan√©ment aux changements.

```
/**
 * ================================================================================
 * ALGORITHME A5 : RULE ENGINE (MONGODB WATCHER)
 * ================================================================================
 * Description :
 * √âcoute les modifications sur la collection 'telemetry_live' et d√©clenche
 * des actions physiques via MQTT si une r√®gle m√©tier est viol√©e.
 * ================================================================================
 */

// Connexion au Cluster VertiFlow
const pipeline = [
    {
        $match: {
            $or: [
                { "fullDocument.leak_detection_status": 1 },
                { "fullDocument.emergency_stop_status": 1 },
                { "fullDocument.irrigation_line_pressure": { $gt: 5.0 } }
            ]
        }
    }
];

const changeStream = db.collection('telemetry_live').watch(pipeline);

changeStream.on('change', (next) => {
    const doc = next.fullDocument;
    const rackID = doc.rack_id;
    const actions = [];

    console.log(`[ALERTE A5] D√©tection √©v√©nement critique sur Rack ${rackID}`);

    // --- R√àGLE R1 : INONDATION ---
    if (doc.leak_detection_status === 1) {
        actions.push({
            topic: `vertiflow/control/${rackID}/pump_main`,
            payload: "OFF",
            reason: "FLOOD_DETECTED"
        });
    }

    // --- R√àGLE R2 : ARR√äT D'URGENCE ---
    if (doc.emergency_stop_status === 1) {
        actions.push({
            topic: `vertiflow/control/${rackID}/power_24v`,
            payload: "CUT",
            reason: "MANUAL_EMERGENCY"
        });
    }

    // --- R√àGLE R3 : SURPRESSION ---
    if (doc.irrigation_line_pressure > 5.0) {
        actions.push({
            topic: `vertiflow/control/${rackID}/valve_relief`,
            payload: "OPEN",
            reason: "OVER_PRESSURE"
        });
    }

    // Ex√©cution des actions (Simulation envoi MQTT)
    actions.forEach(action => {
        print(`--> EXECUTION: Envoi ordre ${action.payload} sur ${action.topic} (${action.reason})`);
        // Ici, on appellerait une fonction publishMQTT(action.topic, action.payload)
    });
    
    // Log de l'incident pour audit
    db.collection('incident_logs').insertOne({
        timestamp: new Date(),
        rack_id: rackID,
        triggers: actions.map(a => a.reason),
        severity: "CRITICAL"
    });
});
```

## 4. Crit√®res de Validation (Definition of Done)

- [ ] Simuler une fuite (`leak_detection_status = 1`) d√©clenche un log "EXECUTION: OFF".
- [ ] Le temps de r√©action entre l'insertion en base et l'action est < 100ms.
- [ ] Une trace de l'incident est cr√©√©e dans la collection `incident_logs`.
- [ ] Le syst√®me g√®re plusieurs alertes simultan√©es (ex: Fuite + Arr√™t Urgence).

# üìâ ALGORITHME A6 : AGR√âGATION TEMPORELLE (DOWNSAMPLING)

**Ticket JIRA :** `TICKET-026` **Responsable :** @Mounir (Architecte) & @Mouhammed (Data Engineer) **Sprint :** Semaine 2 - Phase ETL

## 1. Emplacement dans l'Architecture

Cet algorithme est ex√©cut√© nativement par le moteur de base de donn√©es **ClickHouse** (Zone 5).

- **Entr√©e :** Table brute `basil_ultimate_realtime` (Flux Kafka, ~1 mesure/sec).
- **Sortie :** Vues Mat√©rialis√©es `hourly_stats` et `daily_stats`.
- **Composant Technique :** ClickHouse `Materialized View` + Moteur `AggregatingMergeTree`.

## 2. Description Scientifique & Technique

Les capteurs envoient des donn√©es chaque seconde. Pour une √©tude agronomique sur 3 mois, cela repr√©sente des millions de points, ce qui est trop lourd et "bruyant" pour l'analyse statistique (A7/A8).

### Probl√®me : Le Bruit vs La Tendance

- Une plante ne pousse pas en une seconde.
- Les micro-variations (ex: temp√©rature qui oscille de 0.1¬∞C) n'ont pas d'int√©r√™t biologique direct.
- Le stockage brut co√ªte cher et ralentit les requ√™tes Power BI.

### Solution : L'Agr√©gation Intelligente

L'algorithme A6 r√©duit la r√©solution temporelle tout en conservant les m√©triques statistiques cl√©s pour la science :

- **Moyenne (**$\mu$**) :** Tendance centrale.
- **Min/Max :** Pour d√©tecter les stress extr√™mes (ex: pic de chaleur bref).
- **Quantiles (P95) :** Pour √©liminer les outliers restants.

**Ratio de compression vis√© :** 3600 lignes brutes $\rightarrow$ 1 ligne horaire.

## 3. Impl√©mentation (Script SQL ClickHouse)

Nous utilisons les "Materialized Views" de ClickHouse. Elles calculent l'agr√©gation *au moment de l'insertion*, ce qui rend la lecture instantan√©e.

```
/*
================================================================================
ALGORITHME A6 : VUES D'AGR√âGATION (CLICKHOUSE)
================================================================================
Description :
Cr√©e une vue mat√©rialis√©e qui calcule automatiquement les statistiques horaires
pour chaque Rack et chaque Vari√©t√©.
================================================================================
*/

-- 1. Table de destination (Optimis√©e pour l'agrogation)
CREATE TABLE IF NOT EXISTS smart_farming.basil_hourly_stats (
    timestamp DateTime,
    farm_id LowCardinality(String),
    rack_id LowCardinality(String),
    species_variety LowCardinality(String),
    
    -- M√©triques agr√©g√©es (State)
    avg_temp SimpleAggregateFunction(avg, Float32),
    max_temp SimpleAggregateFunction(max, Float32),
    min_temp SimpleAggregateFunction(min, Float32),
    
    avg_humidity SimpleAggregateFunction(avg, Float32),
    
    total_par_light SimpleAggregateFunction(sum, Float32), -- Somme de la lumi√®re (DLI partiel)
    
    avg_biomass_est SimpleAggregateFunction(avg, Float32)

) ENGINE = AggregatingMergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (farm_id, rack_id, species_variety, timestamp);


-- 2. La Vue Mat√©rialis√©e (Le Trigger de calcul)
CREATE MATERIALIZED VIEW IF NOT EXISTS smart_farming.mv_basil_hourly
TO smart_farming.basil_hourly_stats
AS SELECT
    toStartOfHour(timestamp) as timestamp,
    farm_id,
    rack_id,
    species_variety,
    
    avg(air_temp_internal) as avg_temp,
    max(air_temp_internal) as max_temp,
    min(air_temp_internal) as min_temp,
    
    avg(air_humidity) as avg_humidity,
    
    sum(light_intensity_ppfd) as total_par_light,
    
    avg(fresh_biomass_est) as avg_biomass_est

FROM smart_farming.basil_ultimate_realtime
WHERE data_integrity_flag = 0 -- On n'agr√®ge que les donn√©es valides (Algo A2)
GROUP BY
    toStartOfHour(timestamp),
    farm_id,
    rack_id,
    species_variety;
```

## 4. Crit√®res de Validation (Definition of Done)

- [ ] La table `basil_hourly_stats` se remplit automatiquement d√®s que des donn√©es arrivent dans `basil_ultimate_realtime`.
- [ ] Une requ√™te sur `basil_hourly_stats` est au moins 100x plus rapide que sur la table brute pour une p√©riode d'un mois.
- [ ] Les pics (Max) et les creux (Min) sont conserv√©s (pas de lissage destructeur).
- [ ] Le calcul du DLI (Somme de lumi√®re) est coh√©rent (somme des PPFD instantan√©s).

# üìä ALGORITHME A7 : MATRICE DE CORR√âLATION (PEARSON)

**Ticket JIRA :** `TICKET-022` **Responsable :** @Mounir (Architecte / Scientifique) **Sprint :** Semaine 2 - Phase ETL

## 1. Emplacement dans l'Architecture

Cet algorithme est ex√©cut√© nativement par le moteur de base de donn√©es **ClickHouse** (Zone 5).

- **Entr√©e :** Table `basil_ultimate_realtime` ou Vues Agr√©g√©es (A6).
- **Sortie :** Vue Analytique `view_algo_7_correlation`.
- **Composant Technique :** Fonctions d'agr√©gation `corr()` et `covarPop()`.

## 2. Description Scientifique & Technique

Pour valider une hypoth√®se agronomique, il ne suffit pas de regarder deux courbes. Il faut quantifier leur lien statistique.

### Probl√®me : La Preuve Scientifique

L'agronome pense que *"Plus de CO2 = Plus de croissance"*. Mais si la croissance augmente alors que le CO2 baisse (√† cause d'un autre facteur comme la temp√©rature), l'observation visuelle est trompeuse.

### Solution : Le Coefficient de Corr√©lation de Pearson ($r$)

L'algorithme A7 calcule $r$ pour chaque paire de variables $(X, Y)$ sur une fen√™tre de temps donn√©e.

$$r = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2 \sum(y_i - \bar{y})^2}}$$

- $r \approx 1$ **:** Corr√©lation positive forte (Preuve valid√©e).
- $r \approx -1$ **:** Corr√©lation n√©gative forte (Inversement proportionnel).
- $r \approx 0$ **:** Aucune corr√©lation (Le facteur X n'a aucun effet sur Y).

**Seuil de validation scientifique :** $|r| > 0.8$.

## 3. Impl√©mentation (Script SQL ClickHouse)

Cette vue calcule en temps r√©el la force des liens entre les param√®tres environnementaux et les r√©sultats biologiques.

```
/*
================================================================================
ALGORITHME A7 : MATRICE DE CORR√âLATION (PEARSON)
================================================================================
Description :
Calcule le coefficient de corr√©lation (r) entre les facteurs climatiques/nutritionnels
et les indicateurs de performance (Biomasse, Qualit√©).
Analyse segment√©e par Vari√©t√© pour √©viter les biais g√©n√©tiques.
================================================================================
*/

CREATE OR REPLACE VIEW smart_farming.view_algo_7_correlation AS
SELECT
    species_variety,
    
    -- HYPOTH√àSE 1 : LUMI√àRE vs CROISSANCE
    -- Est-ce que le DLI (Daily Light Integral) pr√©dit la Biomasse ?
    corr(light_dli_accumulated, fresh_biomass_est) AS r_light_growth,
    
    -- HYPOTH√àSE 2 : TEMP√âRATURE vs QUALIT√â
    -- Est-ce que la chaleur d√©truit les huiles essentielles ?
    corr(air_temp_internal, essential_oil_yield) AS r_temp_quality,
    
    -- HYPOTH√àSE 3 : CO2 vs VITESSE
    -- Est-ce que le CO2 acc√©l√®re le taux de croissance relatif (RGR) ?
    corr(co2_level_ambient, relative_growth_rate) AS r_co2_speed,
    
    -- HYPOTH√àSE 4 : NUTRITION K vs SANT√â
    -- Est-ce que le Potassium am√©liore la r√©gulation stomatique ?
    corr(nutrient_k_potassium, stomatal_conductance) AS r_k_stomata,

    -- M√©triques de fiabilit√©
    count() AS sample_size,
    
    -- Interpr√©tation automatique (String)
    multiIf(
        abs(r_light_growth) > 0.8, 'STRONG_LINK',
        abs(r_light_growth) > 0.5, 'MODERATE_LINK',
        'NO_LINK'
    ) as conclusion_light_growth

FROM smart_farming.basil_ultimate_realtime
WHERE 
    timestamp > now() - INTERVAL 30 DAY -- Fen√™tre glissante de 30 jours
    AND data_integrity_flag = 0         -- Donn√©es valides uniquement (Algo A2)
GROUP BY 
    species_variety
ORDER BY 
    species_variety;
```

## 4. Crit√®res de Validation (Definition of Done)

- [ ] La vue `view_algo_7_correlation` retourne un r√©sultat en moins de 1 seconde (m√™me sur 1M lignes).
- [ ] Le coefficient $r$ est toujours compris entre -1 et 1.
- [ ] Si `sample_size` < 100, l'analyse est marqu√©e comme "Non significative" (√† g√©rer dans Power BI).
- [ ] L'exclusion des donn√©es `data_integrity_flag != 0` fonctionne (les outliers ne faussent pas le calcul).

# üß™ ALGORITHME A8 : SEGMENTATION & A/B TESTING (ANOVA)

**Ticket JIRA :** `TICKET-027` **Responsable :** @Mounir (Architecte / Scientifique) **Sprint :** Semaine 2 - Phase ETL

## 1. Emplacement dans l'Architecture

Cet algorithme est ex√©cut√© nativement par le moteur de base de donn√©es **ClickHouse** (Zone 5).

- **Entr√©e :** Table `basil_ultimate_realtime` ou Vues Agr√©g√©es (A6).
- **Sortie :** Vue Analytique `view_algo_8_ab_testing`.
- **Composant Technique :** Fonctions d'agr√©gation `avg()`, `varPop()` (Variance de population) et calculs statistiques d√©riv√©s.

## 2. Description Scientifique & Technique

Pour optimiser la production, l'agronome doit pouvoir tester diff√©rentes "recettes" (combinaisons de lumi√®re, nutriments, etc.) et d√©terminer laquelle est la meilleure.

### Probl√®me : Comparer l'Incomparable

Comparer le rendement du Rack 1 (Recette A) et du Rack 2 (Recette B) ne suffit pas si le Rack 1 a re√ßu plus de lumi√®re naturelle ou si ses plants √©taient plus vieux.

### Solution : L'Analyse de Variance (ANOVA Simplifi√©e)

L'algorithme A8 segmente les donn√©es par `rack_id` (le groupe de test) et calcule pour chaque groupe :

- **Moyenne (**$\mu$**) :** Performance moyenne.
- **Variance (**$\sigma^2$**) :** Dispersions des r√©sultats (Stabilit√©).
- **Intervalle de Confiance (IC) :** Plage dans laquelle se trouve la vraie moyenne √† 95%.

Si les intervalles de confiance de deux racks ne se chevauchent pas, la diff√©rence de performance est **significative**.

## 3. Impl√©mentation (Script SQL ClickHouse)

Cette vue permet de comparer instantan√©ment les performances de diff√©rents racks (groupes de test).

```
/*
================================================================================
ALGORITHME A8 : SEGMENTATION & A/B TESTING
================================================================================
Description :
Compare les performances (Biomasse, Qualit√©) entre diff√©rents groupes (Racks).
Calcule les m√©triques statistiques pour valider si une diff√©rence est significative.
================================================================================
*/

CREATE OR REPLACE VIEW smart_farming.view_algo_8_ab_testing AS
SELECT
    rack_id,
    species_variety,
    
    -- M√©triques de Performance (Moyenne)
    avg(fresh_biomass_est) AS avg_biomass,
    avg(essential_oil_yield) AS avg_oil_yield,
    
    -- M√©triques de Stabilit√© (Variance & Ecart-Type)
    varPop(fresh_biomass_est) AS var_biomass,
    stddevPop(fresh_biomass_est) AS std_biomass,
    
    -- Taille de l'√©chantillon
    count() AS sample_size,
    
    -- Intervalle de Confiance √† 95% (Approximation: 1.96 * Erreur Standard)
    -- CI_Lower = Moyenne - (1.96 * (Ecart-Type / Racine(N)))
    avg_biomass - (1.96 * (std_biomass / sqrt(sample_size))) AS ci_lower_biomass,
    avg_biomass + (1.96 * (std_biomass / sqrt(sample_size))) AS ci_upper_biomass,

    -- Score de Performance Global (Biomasse * Huile)
    avg_biomass * avg_oil_yield AS performance_score

FROM smart_farming.basil_ultimate_realtime
WHERE 
    timestamp > now() - INTERVAL 30 DAY -- Analyse sur le cycle en cours
    AND data_integrity_flag = 0         -- Donn√©es valides uniquement
GROUP BY 
    rack_id,
    species_variety
ORDER BY 
    performance_score DESC;
```

## 4. Crit√®res de Validation (Definition of Done)

- [ ] La vue retourne une ligne par Rack actif.
- [ ] Les intervalles de confiance (`ci_lower`, `ci_upper`) sont calcul√©s.
- [ ] Le tri par `performance_score` permet d'identifier imm√©diatement le meilleur Rack.
- [ ] L'exclusion des donn√©es invalides fonctionne correctement.

# üîÆ ALGORITHME A9 : PR√âDICTION DE R√âCOLTE (LSTM)

**Ticket JIRA :** `TICKET-023` **Responsable :** @Mounir (Scientifique) & @Mouhammed (Data Engineer) **Sprint :** Semaine 2 - Phase ETL

## 1. Emplacement dans l'Architecture

Cet algorithme r√©side dans la couche **Intelligence Pr√©dictive (Zone 5)**, ex√©cut√©e par un moteur Python externe (`oracle.py` dans le dossier `cloud_citadel/nervous_system/`).

- **Entr√©e :** S√©ries temporelles historiques provenant de **ClickHouse** (Table `basil_hourly_stats`).
- **Sortie :** Pr√©dictions de date de r√©colte et de rendement inject√©es dans **Kafka** (Topic `basil_predictions`).
- **Composant Technique :** TensorFlow/Keras (Mod√®le LSTM) + Kafka Producer.

## 2. Description Scientifique & Technique

L'agriculture de pr√©cision ne se limite pas √† observer le pr√©sent ; elle doit anticiper le futur. Pr√©dire la date exacte de r√©colte permet d'optimiser la cha√Æne logistique et de maximiser la fra√Æcheur du produit.

### Probl√®me : La Dynamique Non-Lin√©aire de la Croissance

La croissance d'une plante n'est pas lin√©aire. Elle d√©pend de l'historique cumul√© des conditions environnementales (effet m√©moire). Une baisse de temp√©rature √† J+10 peut retarder la r√©colte de plusieurs jours, m√™me si les conditions redeviennent optimales ensuite.

### Solution : Long Short-Term Memory (LSTM)

Les r√©seaux de neurones r√©currents LSTM sont sp√©cifiquement con√ßus pour apprendre des d√©pendances √† long terme dans des s√©quences temporelles. Le mod√®le prend en entr√©e une fen√™tre glissante des conditions pass√©es (ex: 7 jours) et pr√©dit la biomasse future.

**Variables d'entr√©e (Features) :**

1. `air_temp_internal` (Temp√©rature)
2. `light_dli_accumulated` (Lumi√®re re√ßue)
3. `vapor_pressure_deficit` (Stress hydrique)
4. `co2_level_ambient` (Carbone disponible)
5. `fresh_biomass_est` (Biomasse actuelle estim√©e par vision)

**Variable Cible (Target) :**

- `fresh_biomass_est` √† J+N.

## 3. Impl√©mentation (Code Python - Oracle)

Ce script constitue le c≈ìur du moteur pr√©dictif `oracle.py`.

```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ALGORITHME A9 : ORACLE DE PR√âDICTION DE R√âCOLTE (LSTM)
================================================================================
Description :
Mod√®le de Deep Learning pour pr√©dire la biomasse future du basilic.
Utilise l'historique des capteurs pour estimer la date d'atteinte du poids cible.
================================================================================
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
import logging

# Configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Oracle-A9")

class HarvestPredictor:
    def __init__(self, sequence_length=7, n_features=5):
        """
        Initialise le mod√®le LSTM.
        :param sequence_length: Nombre de jours d'historique (fen√™tre glissante)
        :param n_features: Nombre de variables d'entr√©e (Temp, DLI, VPD, CO2, Biomasse)
        """
        self.seq_len = sequence_length
        self.n_features = n_features
        self.model = self._build_model()
        self.scaler = MinMaxScaler(feature_range=(0, 1))

    def _build_model(self):
        """Construction de l'architecture du r√©seau de neurones."""
        model = Sequential()
        
        # Couche LSTM 1 : Extraction des features temporelles complexes
        model.add(LSTM(units=50, return_sequences=True, input_shape=(self.seq_len, self.n_features)))
        model.add(Dropout(0.2)) # R√©gularisation pour √©viter le sur-apprentissage

        # Couche LSTM 2 : Consolidation de la m√©moire
        model.add(LSTM(units=50, return_sequences=False))
        model.add(Dropout(0.2))

        # Couche Dense de sortie : R√©gression (Pr√©diction de la biomasse future)
        model.add(Dense(units=1))

        model.compile(optimizer='adam', loss='mean_squared_error')
        logger.info("üß† Mod√®le LSTM compil√© avec succ√®s.")
        return model

    def train(self, historical_data):
        """
        Entra√Æne le mod√®le sur les donn√©es historiques ClickHouse.
        :param historical_data: DataFrame Pandas avec les colonnes requises.
        """
        # Pr√©traitement des donn√©es
        data_scaled = self.scaler.fit_transform(historical_data)
        
        X, y = [], []
        for i in range(self.seq_len, len(data_scaled)):
            X.append(data_scaled[i-self.seq_len:i, :]) # S√©quence de 7 jours
            y.append(data_scaled[i, 4]) # Cible : Biomasse (colonne index 4) au jour J
            
        X, y = np.array(X), np.array(y)
        
        logger.info(f"üèãÔ∏è D√©but de l'entra√Ænement sur {len(X)} s√©quences...")
        self.model.fit(X, y, epochs=20, batch_size=32, verbose=1)
        logger.info("‚úÖ Entra√Ænement termin√©.")

    def predict_harvest_date(self, current_sequence, target_weight):
        """
        Simule la croissance future jour par jour jusqu'√† atteindre la cible.
        
        :param current_sequence: Array (1, 7, 5) - Les 7 derniers jours r√©els.
        :param target_weight: Float - Poids cible (ex: 50g).
        :return: Dict avec jours restants et date estim√©e.
        """
        # Normalisation de l'entr√©e
        # Note : En prod, il faut g√©rer le scaler pour ne transformer que les inputs
        # Ici on simplifie pour la logique algorithmique
        
        days_remaining = 0
        predicted_weight = current_sequence[0, -1, 4] # Poids actuel (Dernier jour)
        simulated_seq = current_sequence.copy()
        
        MAX_DAYS = 45 # S√©curit√© pour √©viter boucle infinie
        
        logger.info(f"üîÆ Simulation de croissance. Poids actuel: {predicted_weight:.2f}g -> Cible: {target_weight}g")

        while predicted_weight < target_weight and days_remaining < MAX_DAYS:
            # Pr√©diction pour J+1
            next_step_scaled = self.model.predict(simulated_seq, verbose=0)
            
            # Mise √† jour du poids pr√©dit (D√©normalisation approximative pour l'exemple)
            # Dans la r√©alit√©, on utiliserait self.scaler.inverse_transform()
            growth_factor = next_step_scaled[0][0] 
            predicted_weight += growth_factor # Hypoth√®se simplifi√©e : mod√®le pr√©dit le gain
            
            days_remaining += 1
            
            # Mise √† jour de la s√©quence glissante pour le pas suivant
            # On d√©cale les jours et on ajoute la nouvelle pr√©diction en fin
            # On suppose ici des conditions climatiques stables (moyenne des 7 derniers jours)
            new_day = np.mean(simulated_seq[0], axis=0) 
            new_day[4] = predicted_weight # Mise √† jour de la biomasse
            
            # Rotation : [J-6 ... J] -> [J-5 ... J+1]
            simulated_seq = np.append(simulated_seq[:, 1:, :], [[new_day]], axis=1)

        return {
            "rack_id": "R01", # √Ä dynamiser
            "days_remaining": days_remaining,
            "predicted_final_biomass": float(predicted_weight),
            "status": "ON_TRACK" if days_remaining < 20 else "DELAYED"
        }

# --- Bloc de test unitaire ---
if __name__ == "__main__":
    # G√©n√©ration de donn√©es factices pour tester la logique
    mock_data = pd.DataFrame(np.random.rand(100, 5), columns=['temp', 'dli', 'vpd', 'co2', 'biomass'])
    
    oracle = HarvestPredictor()
    oracle.train(mock_data)
    
    # Test de pr√©diction sur une s√©quence al√©atoire
    test_seq = np.random.rand(1, 7, 5)
    result = oracle.predict_harvest_date(test_seq, target_weight=1.5)
    
    print(f"\nR√âSULTAT ORACLE : {result}")
```

## 4. Crit√®res de Validation (Definition of Done)

- [ ] Le mod√®le converge lors de l'entra√Ænement (la perte `loss` diminue).
- [ ] La fonction `predict_harvest_date` retourne un nombre de jours coh√©rent (pas n√©gatif, pas infini).
- [ ] Le script peut charger des donn√©es depuis un fichier CSV ou une requ√™te ClickHouse simul√©e.
- [ ] Les d√©pendances (`tensorflow`, `pandas`, `scikit-learn`) sont bien list√©es dans `requirements.txt`.

# üß† ALGORITHME A11 : OPTIMISATION DE RECETTE (GRADIENT DESCENT)

**Ticket JIRA :** `TICKET-029` **Responsable :** @Mounir (Scientifique) & @Mounir (Architecte) **Sprint :** Semaine 3 - Phase Intelligence

## 1. Emplacement dans l'Architecture

Cet algorithme r√©side dans le **Cerveau Sup√©rieur (Cortex)** (Zone 5), module `cortex.py`. Il ferme la boucle de r√©troaction en proposant de nouveaux param√®tres de consigne.

- **Entr√©e :** R√©sultats des cycles pr√©c√©dents (ClickHouse - `basil_hourly_stats` et `harvest_results`).
- **Sortie :** Nouvelles cibles optimales (`ref_n_target`, `ref_temp_opt`, etc.) mises √† jour dans MongoDB.
- **Composant Technique :** Scikit-Learn / Scipy (Optimisation).

## 2. Description Scientifique & Technique

L'objectif est de trouver la combinaison id√©ale de param√®tres environnementaux ("Recette") qui maximise le rendement ou la rentabilit√©, sans intervention humaine constante.

### Probl√®me : L'Espace de Recherche Vaste

Il existe une infinit√© de combinaisons possibles de Temp√©rature, Lumi√®re, EC, pH, etc. Tester chaque combinaison manuellement prendrait des si√®cles.

### Solution : Descente de Gradient (ou Algorithme G√©n√©tique)

L'algorithme mod√©lise la fonction de rendement $f(x_1, x_2, ... x_n)$ o√π $x_i$ sont les param√®tres contr√¥lables. Il cherche ensuite les valeurs de $x$ qui maximisent $f(x)$ en suivant la pente ascendante (Gradient Ascent).

**Fonction Objectif (√† maximiser) :**

$$Score = \alpha \times \text{Yield} + \beta \times \text{Quality} - \gamma \times \text{Cost}$$

## 3. Impl√©mentation (Code Python - Cortex)

```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
ALGORITHME A11 : OPTIMISATEUR DE RECETTE (CORTEX)
================================================================================
Description :
Analyse les performances pass√©es pour recommander des ajustements
des param√®tres de consigne (Cibles) afin de maximiser le Score de Performance.
================================================================================
"""

import numpy as np
import pandas as pd
from scipy.optimize import minimize
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
import logging

# Configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Cortex-A11")

class RecipeOptimizer:
    def __init__(self):
        # Mod√®le de substitution (Surrogate Model) pour estimer la fonction de rendement
        # On utilise une r√©gression polynomiale pour capturer les non-lin√©arit√©s
        self.model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())
        self.param_bounds = [
            (18.0, 28.0),  # Temp√©rature (¬∞C)
            (10.0, 20.0),  # DLI (mol/m¬≤/d)
            (1.2, 2.5)     # EC (mS/cm)
        ]
        self.feature_names = ['avg_temp', 'dli', 'avg_ec']

    def train_surrogate_model(self, historical_data):
        """
        Entra√Æne un mod√®le simple qui pr√©dit le score en fonction des param√®tres.
        :param historical_data: DataFrame avec colonnes features + 'performance_score'.
        """
        X = historical_data[self.feature_names]
        y = historical_data['performance_score']
        
        self.model.fit(X, y)
        r2 = self.model.score(X, y)
        logger.info(f"üß† Mod√®le de substitution entra√Æn√©. R2: {r2:.3f}")

    def _objective_function(self, params):
        """
        Fonction √† maximiser (le mod√®le pr√©dit le score).
        Note: minimize cherche le minimum, donc on retourne -score pour maximiser.
        """
        # Reshape pour faire une pr√©diction unique
        params_reshaped = np.array(params).reshape(1, -1)
        predicted_score = self.model.predict(params_reshaped)[0]
        return -predicted_score

    def find_optimal_recipe(self, current_recipe):
        """
        Cherche la meilleure recette √† partir du point actuel.
        :param current_recipe: Liste [temp, dli, ec] actuels.
        """
        logger.info(f"üîç Recherche de l'optimum local depuis {current_recipe}...")
        
        result = minimize(
            self._objective_function,
            x0=current_recipe,
            bounds=self.param_bounds,
            method='L-BFGS-B' # M√©thode efficace pour probl√®mes born√©s
        )

        if result.success:
            optimized_params = result.x
            predicted_gain = -result.fun
            logger.info(f"üöÄ Optimum trouv√© : {optimized_params}")
            logger.info(f"üìà Score pr√©dit : {predicted_gain:.2f}")
            
            return {
                "new_targets": {
                    "ref_temp_opt": round(optimized_params[0], 1),
                    "ref_dli_target": round(optimized_params[1], 1),
                    "ref_ec_target": round(optimized_params[2], 1)
                },
                "predicted_score": predicted_gain,
                "confidence": "HIGH" # Simplifi√©
            }
        else:
            logger.warning("‚ö†Ô∏è L'optimisation a √©chou√©.")
            return None

# --- Bloc de test unitaire ---
if __name__ == "__main__":
    # Donn√©es factices : On suppose que le score est max vers 24¬∞C, 15 DLI, 1.8 EC
    # Formule : Score = 100 - (T-24)^2 - (DLI-15)^2 - 10*(EC-1.8)^2
    data_size = 50
    X_mock = np.random.uniform(low=[18, 10, 1.2], high=[28, 20, 2.5], size=(data_size, 3))
    y_mock = 100 - (X_mock[:,0]-24)**2 - (X_mock[:,1]-15)**2 - 10*(X_mock[:,2]-1.8)**2
    
    df = pd.DataFrame(X_mock, columns=['avg_temp', 'dli', 'avg_ec'])
    df['performance_score'] = y_mock

    optimizer = RecipeOptimizer()
    optimizer.train_surrogate_model(df)
    
    current = [20.0, 12.0, 1.5] # Point de d√©part sous-optimal
    res = optimizer.find_optimal_recipe(current)
    print(f"\nüí° RECOMMANDATION CORTEX : {res}")
```

## 4. Crit√®res de Validation

- [ ] Le "Surrogate Model" capture correctement la tendance des donn√©es (R2 raisonnable).
- [ ] L'optimiseur propose des valeurs dans les bornes d√©finies (pas de T > 28¬∞C).
- [ ] L'algorithme converge vers un meilleur score que le point de d√©part.
- [ ] Les recommandations sont au format JSON pr√™t √† √™tre envoy√© √† MongoDB (Mise √† jour des cibles).