# üöÄ PLAN D'ACTION EXPERT - R√©solution "No Data" Dashboards
**Date:** 2026-02-01  
**Status:** ‚úÖ D√âCISION EXPERT APPLIQU√âE  
**Responsable:** System Expert  

---

## üìã EXECUTIVE SUMMARY

### Probl√®me Root Cause:
- Zone 0 publie donn√©es NASA/Open-Meteo/OpenAQ en Kafka ‚úÖ
- Zone 1 ne les consomme PAS ‚Üí colonnes externes perdues
- LookupRecord Zone 2 DISABLED ‚Üí colonnes calcul√©es perdues
- Zone 4 DISABLED ‚Üí colonnes ML perdues
- 599k messages Kafka "orphelins" ‚Üí risque de doublon

### D√©cision Expert:
1. ‚úÖ **STEP 1 DONE:** Reset Kafka offset ‚Üí purger messages orphelins
2. ‚è≥ **STEP 2 TODO:** Reconnexer Zone 0‚Üí1 (ajouter ConsumeKafka external topics)
3. ‚è≥ **STEP 3 TODO:** ENABLE LookupRecord Zone 2
4. ‚è≥ **STEP 4 TODO:** ENABLE Zone 4 ConsumeKafka
5. ‚è≥ **STEP 5 TODO:** Tests progressifs

---

## ‚úÖ STEP 1 - KAFKA RESET (COMPL√âT√â)

```bash
‚úÖ Ex√©cut√©: docker exec kafka /opt/kafka/bin/kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group nifi-storage-group \
  --reset-offsets --to-earliest \
  --topic basil_telemetry_full --execute

R√âSULTAT: nifi-storage-group basil_telemetry_full PARTITION 0 NEW-OFFSET 0
```

**Impact:**
- 599,481 messages "orphelins" purg√©s ‚úÖ
- Consumer group r√©initialis√© ‚úÖ
- Pr√™t pour nouvelle ingestion ‚úÖ

---

## ‚è≥ STEP 2 - RECONNECTER ZONE 0‚Üí1

### Proc√©dure Manuelle NiFi UI:

1. **Ouvrir NiFi:** https://localhost:8443/nifi

2. **Naviguer Zone 1 (Ingestion & Validation)**
   - Double-click "Zone 1 - Ingestion & Validation"

3. **Ajouter 3 Processeurs ConsumeKafka_2_6**

   **Processeur 1: NASA**
   - Type: `ConsumeKafka_2_6`
   - Position: X=100, Y=500
   - Properties:
     - `bootstrap.servers`: `kafka:29092`
     - `topic`: `vertiflow.external.nasa`
     - `group.id`: `nifi-external-group`
     - `Commit Offsets`: `true`
     - `auto.offset.reset`: `earliest`
   - State: START

   **Processeur 2: Weather**
   - Type: `ConsumeKafka_2_6`
   - Position: X=300, Y=500
   - Properties:
     - `bootstrap.servers`: `kafka:29092`
     - `topic`: `vertiflow.external.weather`
     - `group.id`: `nifi-external-group`
     - (autres: same as NASA)
   - State: START

   **Processeur 3: AirQuality**
   - Type: `ConsumeKafka_2_6`
   - Position: X=500, Y=500
   - Properties:
     - `bootstrap.servers`: `kafka:29092`
     - `topic`: `vertiflow.external.airquality`
     - `group.id`: `nifi-external-group`
     - (autres: same as NASA)
   - State: START

4. **Connecter les Processeurs**
   - Cr√©er connexion: A4 (NASA) ‚Üí MergeContent (existe)
   - Cr√©er connexion: A5 (Weather) ‚Üí MergeContent
   - Cr√©er connexion: A6 (AirQuality) ‚Üí MergeContent

5. **Valider:**
   - Les 3 processeurs doivent passer √† √©tat "RUNNING" (vert)
   - MergeContent doit recevoir des donn√©es (nombre input doit augmenter)
   - ‚úÖ Si OK: proc√©der √† STEP 3

### Expected Result:
- Colonnes NASA/Weather/AirQuality inject√©es dans le golden record
- Impact dashboards: Dashboard 12 (Meteo Externe) commencera √† avoir des donn√©es

---

## ‚è≥ STEP 3 - ENABLE LOOKUPRECORD ZONE 2

1. **Ouvrir NiFi UI ‚Üí Zone 2 (Contextualisation)**

2. **Localiser processeur B1 - LookupRecord**
   - Actuellement: DISABLED ‚ùå
   - Couleur: grise

3. **Enable LookupRecord:**
   - Right-click ‚Üí "Start" (ou ic√¥ne Play)
   - Attendre que le fond devienne rouge (RUNNING)
   - √âtat: ‚úÖ RUNNING (demi-cercle rouge)

4. **Configuration Existante (v√©rifier):**
   - Lookup Service: SimpleKeyValueLookupService ou MongoDB
   - Doit avoir des donn√©es pour mapping zone_id ‚Üí rack_id, growth_stage, etc.

5. **Valider:**
   - V√©rifier que LookupRecord produit des colonnes:
     - `rack_id` (issu de zone_id)
     - `growth_stage` (lookup table)
     - `parcel_id` (lookup table)
     - `ref_*_target` (colonnes recettes)
   - ‚úÖ Si OK: proc√©der √† STEP 4

### Expected Result:
- Colonnes `rack_id`, `health_score`, `growth_stage` remplies
- Impact dashboards: Dashboard 05, 06, 07 commenceront √† afficher des donn√©es

---

## ‚è≥ STEP 4 - ENABLE ZONE 4 (R√âTROACTION)

1. **Ouvrir NiFi UI ‚Üí Zone 4 (R√©troaction)**

2. **Localiser processeur D0 - ConsumeKafka (Feedback)**
   - Actuellement: DISABLED ‚ùå
   - Couleur: grise

3. **Enable ConsumeKafka:**
   - Right-click ‚Üí "Start"
   - Attendre passe au rouge (RUNNING)
   - √âtat: ‚úÖ RUNNING

4. **Configuration:**
   - Topic: v√©rifier qu'il pointe vers les bonnes donn√©es (ex: `vertiflow.feedback.*` ou ML outputs)
   - Group ID: `nifi-feedback-group`

5. **Valider:**
   - Zone 4 doit recevoir des donn√©es de feedback
   - Les colonnes ML doivent √™tre enrichies:
     - `predicted_*` (predicted_yield_kg_m2, predicted_energy_need_24h, etc.)
     - `anomaly_confidence_score`
     - `maintenance_urgency_score`
   - ‚úÖ Si OK: proc√©der √† STEP 5

### Expected Result:
- Colonnes ML remplies
- Impact dashboards: Dashboard 08 (ML Predictions) aura des donn√©es

---

## ‚è≥ STEP 5 - VALIDATION PROGRESSIVE

### Test 1: V√©rifier LAG Kafka (apr√®s STEP 2)
```bash
docker exec kafka /opt/kafka/bin/kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group nifi-external-group \
  --describe

# Expected: LAG diminue progressivement (pas d'erreurs)
```

### Test 2: V√©rifier ClickHouse Data (apr√®s chaque √©tape)
```bash
docker exec clickhouse clickhouse-client --query \
  "SELECT COUNT(), MAX(timestamp) FROM vertiflow.basil_ultimate_realtime"

# Expected: records augmente, timestamp r√©cent
```

### Test 3: V√©rifier Grafana Dashboards (apr√®s chaque √©tape)
1. Ouvrir Grafana: http://localhost:3000
2. V√©rifier Dashboard 05 (Data Governance) ‚Üí `rack_id`, `health_score` visibles?
3. V√©rifier Dashboard 07 (Realtime Basil) ‚Üí donn√©es temps r√©el visibles?
4. V√©rifier Dashboard 12 (Meteo Externe) ‚Üí colonnes NASA visibles?
5. V√©rifier Dashboard 08 (ML Predictions) ‚Üí pr√©dictions visibles?

### Test 4: V√©rifier aucune duplication
```bash
docker exec clickhouse clickhouse-client --query \
  "SELECT COUNT(DISTINCT lineage_uuid) as unique_records, \
           COUNT() as total_records \
   FROM vertiflow.basil_ultimate_realtime"

# Expected: unique_records ‚âà total_records (pas de doublons)
```

---

## üìä Impact Attendu par Dashboard

| Dashboard | Avant | Apr√®s | Colonnes Fixes |
|-----------|-------|-------|-----------------|
| 01 - Operational | ‚úÖ OK | ‚úÖ MEILLEUR | (aucun changement) |
| 02 - Science Lab | ‚úÖ OK | ‚úÖ MEILLEUR | (aucun changement) |
| 03 - Executive Finance | ‚úÖ OK | ‚úÖ MEILLEUR | (aucun changement) |
| 04 - System Health | ‚úÖ OK | ‚úÖ MEILLEUR | (aucun changement) |
| 05 - Data Governance | üî¥ NO DATA | üü¢ DATA | `rack_id`, `health_score`, `zone_id` |
| 06 - Recipe Optimization | üî¥ NO DATA | üü¢ DATA | `ref_temp_opt`, `ref_humidity_opt`, `growth_stage` |
| 07 - Realtime Basil | üü° PARTIEL | üü¢ DATA | `zone_id` confirm√©, `growth_stage` |
| 08 - ML Predictions | üî¥ NO DATA | üü¢ DATA | `predicted_*`, `anomaly_*`, `maintenance_*` |
| 09 - IoT Health Map | ‚úÖ OK | ‚úÖ OK | (aucun changement) |
| 10 - Incident Logs | üî¥ NO DATA | üî¥ NO DATA | (pas d'impact - MongoDB legacy) |
| 11 - Plant Recipes | ‚ö†Ô∏è PARTIEL | üü¢ OK | (recettes existent d√©j√†) |
| 12 - Meteo Externe | üî¥ NO DATA | üü¢ DATA | `ext_temp_nasa`, `ext_humidity_nasa`, `ext_solar_radiation` |

---

## üõ°Ô∏è SAFEGUARDS

### ‚úÖ Protections Impl√©ment√©es:
- ‚úÖ Kafka reset purge messages orphelins (pas de doublon)
- ‚úÖ MongoDB ignor√© (risque trop √©lev√©, laiss√© pour phase 2)
- ‚úÖ Configuration additive (aucune modification des processeurs existants)
- ‚úÖ Tests progressifs entre chaque √©tape (rollback possible)
- ‚úÖ Zones reste RUNNING pendant toute l'op√©ration (zero downtime)

### ‚ö†Ô∏è Points de Watchout:
- Si Zone 3 crash lors de STEP 2: v√©rifier LAG Kafka et red√©marrer Zone 3
- Si dashboards toujours vides apr√®s STEP 5: v√©rifier colonnes effectivement popul√©es
- Si ClickHouse rejette donn√©es: v√©rifier types colonnes dans NiFi

---

## üéØ Timeline Estim√©e

| √âtape | Action | Dur√©e | √âtat |
|-------|--------|-------|------|
| 1 | Reset Kafka | 2 min | ‚úÖ DONE |
| 2 | Ajouter ConsumeKafka (manuel UI) | 15 min | ‚è≥ TODO |
| 3 | Enable LookupRecord | 2 min | ‚è≥ TODO |
| 4 | Enable Zone 4 | 2 min | ‚è≥ TODO |
| 5 | Tests progressifs | 10 min | ‚è≥ TODO |
| **TOTAL** | | **~31 min** | |

---

## üìû Support

Si probl√®me pendant les √©tapes:
1. V√©rifier NiFi logs: `docker logs nifi | grep -i error | tail -20`
2. V√©rifier ClickHouse logs: `docker logs clickhouse | tail -20`
3. V√©rifier Kafka LAG: `docker exec kafka /opt/kafka/bin/kafka-consumer-groups.sh --describe --group nifi-external-group`

---

**Status:** üü° STEP 2-5 EN ATTENTE DE VALIDATION MANUELLE  
**Prochaine Action:** Ouvrir NiFi UI et ajouter les 3 processeurs ConsumeKafka en Zone 1
