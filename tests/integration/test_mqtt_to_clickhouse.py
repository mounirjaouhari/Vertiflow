#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
PROJET VERTIFLOW - Agriculture Verticale Intelligente
================================================================================
Date de crÃ©ation    : 02/01/2026
Ã‰quipe              : VertiFlow Core Team

Membres de l'Ã©quipe :
    ğŸ§™â€â™‚ï¸ Mounir      - Architecte & Scientifique (Python Dev)
    ğŸ—ï¸ Imrane      - DevOps & Infrastructure (Python Dev)
    ğŸ Mouhammed   - Data Engineer & Analyste ETL
    ğŸ§¬ Asama       - Biologiste & Domain Expert (Python Dev)
    âš–ï¸ MrZakaria   - Encadrant & Architecte Data

--------------------------------------------------------------------------------
MODULE: tests/integration/test_mqtt_to_clickhouse.py
DESCRIPTION: Tests d'intÃ©gration du pipeline MQTT â†’ Kafka â†’ ClickHouse

    Ce test vÃ©rifie le pipeline COMPLET de collecte de donnÃ©es IoT:
    
    FLUX DE DONNÃ‰ES:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     PIPELINE IoT - TEST D'INTÃ‰GRATION                   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
    â”‚  â”‚   CAPTEUR    â”‚  Ce test simule un capteur IoT                        â”‚
    â”‚  â”‚   (simulÃ©)   â”‚  qui publie sur MQTT                                  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
    â”‚         â”‚                                                               â”‚
    â”‚         â”‚ MQTT Publish                                                  â”‚
    â”‚         â”‚ Topic: vertiflow/sensors/R01                                  â”‚
    â”‚         â–¼                                                               â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
    â”‚  â”‚  MOSQUITTO   â”‚  Broker MQTT                                          â”‚
    â”‚  â”‚  (MQTT)      â”‚  Port: 1883                                           â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
    â”‚         â”‚                                                               â”‚
    â”‚         â”‚ NiFi/Kafka Connect                                            â”‚
    â”‚         â”‚ (ou script de bridge)                                         â”‚
    â”‚         â–¼                                                               â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
    â”‚  â”‚   KAFKA      â”‚  Message Queue                                        â”‚
    â”‚  â”‚              â”‚  Topic: vertiflow.sensor                              â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
    â”‚         â”‚                                                               â”‚
    â”‚         â”‚ Stream Processor                                              â”‚
    â”‚         â”‚ (Consumer Kafka)                                              â”‚
    â”‚         â–¼                                                               â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
    â”‚  â”‚ CLICKHOUSE   â”‚  Stockage Time-Series                                 â”‚
    â”‚  â”‚              â”‚  Table: sensor_data                                   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    OBJECTIF DU TEST:
    VÃ©rifier que les donnÃ©es publiÃ©es sur MQTT arrivent bien dans ClickHouse
    avec les bonnes valeurs et dans un dÃ©lai acceptable (< 5 secondes).

    PRÃ‰REQUIS:
    Les services suivants doivent Ãªtre dÃ©marrÃ©s:
        $ docker-compose up -d mosquitto kafka clickhouse
    
    EXÃ‰CUTION:
        $ pytest tests/integration/test_mqtt_to_clickhouse.py -v

DÃ©veloppÃ© par       : @Imrane & @Mouhammed
Ticket(s) associÃ©(s): TICKET-110
Sprint              : Semaine 6 - Phase QualitÃ© & Tests

DÃ©pendances:
    - pytest>=8.0.0
    - paho-mqtt>=2.0.0
    - clickhouse-driver>=0.2.6
    - kafka-python>=2.0.2

================================================================================
Â© 2026 VertiFlow Core Team - Tous droits rÃ©servÃ©s
DÃ©veloppÃ© dans le cadre de l'Initiative Nationale Marocaine JobInTech
au sein de l'Ã‰cole YNOV Maroc Campus
================================================================================
"""

import os
import json
import time
import uuid
import pytest
from datetime import datetime, timezone
from typing import Dict, Any, Optional

# Import des utilitaires d'intÃ©gration
from tests.integration import (
    require_services,
    wait_for_condition,
    get_clickhouse_client,
    get_mqtt_client,
    get_kafka_producer,
    get_kafka_consumer,
    is_port_open,
    logger,
    CLICKHOUSE_HOST,
    CLICKHOUSE_PORT,
    MQTT_HOST,
    MQTT_PORT,
    KAFKA_BOOTSTRAP_SERVERS,
)


# =============================================================================
# CONSTANTES DE TEST
# =============================================================================

# Topics MQTT
MQTT_TOPIC_SENSORS = "vertiflow/sensors/{rack_id}"
MQTT_TOPIC_SENSORS_WILDCARD = "vertiflow/sensors/#"

# Topics Kafka
KAFKA_TOPIC_SENSOR_DATA = "vertiflow.sensor"

# Table ClickHouse
CLICKHOUSE_TABLE_SENSOR = "sensor_data"
CLICKHOUSE_DATABASE = "vertiflow_test"

# Timeouts
MESSAGE_PROPAGATION_TIMEOUT = 10.0  # Max 10s pour la propagation
POLL_INTERVAL = 0.5

# Identifiants de test (uniques pour Ã©viter les collisions)
TEST_RUN_ID = str(uuid.uuid4())[:8]


# =============================================================================
# MARQUEURS PYTEST
# =============================================================================

# Marquer tous les tests comme tests d'intÃ©gration
pytestmark = [
    pytest.mark.integration,
    pytest.mark.requires_docker,
]


# =============================================================================
# FIXTURES
# =============================================================================

@pytest.fixture(scope="module")
def check_services():
    """
    VÃ©rifie que tous les services nÃ©cessaires sont disponibles.
    Skip le module entier si un service manque.
    """
    services_status = {
        "mqtt": is_port_open(MQTT_HOST, MQTT_PORT, timeout=2),
        "clickhouse": is_port_open(CLICKHOUSE_HOST, CLICKHOUSE_PORT, timeout=2),
        "kafka": is_port_open(
            KAFKA_BOOTSTRAP_SERVERS.split(":")[0],
            int(KAFKA_BOOTSTRAP_SERVERS.split(":")[1]),
            timeout=2
        ),
    }
    
    missing = [name for name, available in services_status.items() if not available]
    
    if missing:
        pytest.skip(
            f"Services manquants: {', '.join(missing)}. "
            f"DÃ©marrez avec: docker-compose up -d mosquitto kafka clickhouse"
        )
    
    logger.info("âœ… Tous les services sont disponibles pour les tests d'intÃ©gration")
    return services_status


@pytest.fixture(scope="module")
def clickhouse_client(check_services):
    """
    Client ClickHouse pour les tests.
    CrÃ©e la table de test si elle n'existe pas.
    """
    client = get_clickhouse_client()
    
    # CrÃ©er la base de test si nÃ©cessaire
    client.execute(f"CREATE DATABASE IF NOT EXISTS {CLICKHOUSE_DATABASE}")
    client.execute(f"USE {CLICKHOUSE_DATABASE}")
    
    # CrÃ©er la table de test
    client.execute(f"""
        CREATE TABLE IF NOT EXISTS {CLICKHOUSE_TABLE_SENSOR} (
            timestamp DateTime64(3) DEFAULT now64(3),
            rack_id String,
            test_run_id String,
            temperature_c Float32,
            humidity_pct Float32,
            co2_ppm UInt16,
            light_ppfd UInt16,
            ec_ms_cm Float32,
            ph Float32,
            water_temp_c Float32
        ) ENGINE = MergeTree()
        ORDER BY (rack_id, timestamp)
        TTL timestamp + INTERVAL 1 DAY
    """)
    
    logger.info(f"âœ… Table ClickHouse crÃ©Ã©e: {CLICKHOUSE_DATABASE}.{CLICKHOUSE_TABLE_SENSOR}")
    
    yield client
    
    # Cleanup: supprimer les donnÃ©es de ce test run
    try:
        client.execute(f"""
            ALTER TABLE {CLICKHOUSE_TABLE_SENSOR} 
            DELETE WHERE test_run_id = '{TEST_RUN_ID}'
        """)
        logger.info(f"ğŸ—‘ï¸ DonnÃ©es de test nettoyÃ©es (run_id: {TEST_RUN_ID})")
    except Exception as e:
        logger.warning(f"Erreur lors du nettoyage: {e}")


@pytest.fixture(scope="module")
def mqtt_client(check_services):
    """
    Client MQTT pour publier les messages de test.
    """
    client = get_mqtt_client()
    client.loop_start()  # DÃ©marrer la boucle de traitement en arriÃ¨re-plan
    
    yield client
    
    client.loop_stop()
    client.disconnect()
    logger.info("ğŸ”Œ Client MQTT dÃ©connectÃ©")


@pytest.fixture(scope="module")
def kafka_producer(check_services):
    """
    Producer Kafka pour les tests directs Kafka â†’ ClickHouse.
    """
    producer = get_kafka_producer()
    
    yield producer
    
    producer.close()
    logger.info("ğŸ”Œ Producer Kafka fermÃ©")


@pytest.fixture
def unique_rack_id():
    """
    GÃ©nÃ¨re un rack_id unique pour chaque test.
    Ã‰vite les collisions entre tests parallÃ¨les.
    """
    return f"R_TEST_{TEST_RUN_ID}_{uuid.uuid4().hex[:6]}"


@pytest.fixture
def sensor_data_factory(unique_rack_id):
    """
    Factory pour crÃ©er des donnÃ©es de capteurs.
    """
    def _create_sensor_data(
        rack_id: str = None,
        temperature: float = 24.5,
        humidity: float = 68.0,
        co2: int = 850,
        light: int = 420,
        ec: float = 1.75,
        ph: float = 6.2,
        water_temp: float = 21.0
    ) -> Dict[str, Any]:
        return {
            "rack_id": rack_id or unique_rack_id,
            "test_run_id": TEST_RUN_ID,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "sensors": {
                "temperature_c": temperature,
                "humidity_pct": humidity,
                "co2_ppm": co2,
                "light_ppfd": light,
                "ec_ms_cm": ec,
                "ph": ph,
                "water_temp_c": water_temp
            }
        }
    
    return _create_sensor_data


# =============================================================================
# FONCTIONS UTILITAIRES
# =============================================================================

def insert_directly_to_clickhouse(client, data: Dict[str, Any]) -> bool:
    """
    InsÃ¨re les donnÃ©es directement dans ClickHouse (bypass du pipeline).
    UtilisÃ© pour tester la lecture.
    """
    try:
        sensors = data.get("sensors", {})
        client.execute(f"""
            INSERT INTO {CLICKHOUSE_TABLE_SENSOR} 
            (rack_id, test_run_id, temperature_c, humidity_pct, co2_ppm, 
             light_ppfd, ec_ms_cm, ph, water_temp_c)
            VALUES
        """, [{
            "rack_id": data["rack_id"],
            "test_run_id": data.get("test_run_id", TEST_RUN_ID),
            "temperature_c": sensors.get("temperature_c", 0),
            "humidity_pct": sensors.get("humidity_pct", 0),
            "co2_ppm": sensors.get("co2_ppm", 0),
            "light_ppfd": sensors.get("light_ppfd", 0),
            "ec_ms_cm": sensors.get("ec_ms_cm", 0),
            "ph": sensors.get("ph", 0),
            "water_temp_c": sensors.get("water_temp_c", 0),
        }])
        return True
    except Exception as e:
        logger.error(f"Erreur insertion ClickHouse: {e}")
        return False


def query_sensor_data(client, rack_id: str, test_run_id: str = TEST_RUN_ID) -> list:
    """
    RequÃªte les donnÃ©es de capteurs dans ClickHouse.
    """
    result = client.execute(f"""
        SELECT 
            rack_id,
            temperature_c,
            humidity_pct,
            co2_ppm,
            light_ppfd,
            ec_ms_cm,
            ph,
            water_temp_c,
            timestamp
        FROM {CLICKHOUSE_TABLE_SENSOR}
        WHERE rack_id = %(rack_id)s
          AND test_run_id = %(test_run_id)s
        ORDER BY timestamp DESC
        LIMIT 10
    """, {"rack_id": rack_id, "test_run_id": test_run_id})
    
    return result


def count_sensor_records(client, rack_id: str, test_run_id: str = TEST_RUN_ID) -> int:
    """
    Compte le nombre d'enregistrements pour un rack.
    """
    result = client.execute(f"""
        SELECT count() 
        FROM {CLICKHOUSE_TABLE_SENSOR}
        WHERE rack_id = %(rack_id)s
          AND test_run_id = %(test_run_id)s
    """, {"rack_id": rack_id, "test_run_id": test_run_id})
    
    return result[0][0] if result else 0


# =============================================================================
# TESTS D'INTÃ‰GRATION
# =============================================================================

class TestMQTTToClickHouse:
    """
    Tests du pipeline MQTT â†’ Kafka â†’ ClickHouse.
    """
    
    # -------------------------------------------------------------------------
    # TEST 1: Connexion MQTT
    # -------------------------------------------------------------------------
    def test_mqtt_connection(self, mqtt_client):
        """
        Test: connexion au broker MQTT rÃ©ussie.
        """
        # ASSERT - Le client est connectÃ© (fixture rÃ©ussie)
        assert mqtt_client is not None, "Client MQTT non crÃ©Ã©"
        assert mqtt_client.is_connected(), "Client MQTT non connectÃ©"
        
        logger.info("âœ… Connexion MQTT vÃ©rifiÃ©e")
    
    # -------------------------------------------------------------------------
    # TEST 2: Publication MQTT
    # -------------------------------------------------------------------------
    def test_mqtt_publish(self, mqtt_client, sensor_data_factory, unique_rack_id):
        """
        Test: publication d'un message MQTT rÃ©ussie.
        """
        # ARRANGE
        data = sensor_data_factory(rack_id=unique_rack_id)
        topic = MQTT_TOPIC_SENSORS.format(rack_id=unique_rack_id)
        payload = json.dumps(data)
        
        # ACT
        result = mqtt_client.publish(topic, payload, qos=1)
        result.wait_for_publish(timeout=5)
        
        # ASSERT
        assert result.is_published(), "Message non publiÃ©"
        assert result.rc == 0, f"Code retour MQTT non nul: {result.rc}"
        
        logger.info(f"âœ… Message publiÃ© sur {topic}")
    
    # -------------------------------------------------------------------------
    # TEST 3: Connexion ClickHouse
    # -------------------------------------------------------------------------
    def test_clickhouse_connection(self, clickhouse_client):
        """
        Test: connexion Ã  ClickHouse rÃ©ussie.
        """
        # ACT
        result = clickhouse_client.execute("SELECT 1")
        
        # ASSERT
        assert result == [(1,)], "RequÃªte ClickHouse Ã©chouÃ©e"
        
        logger.info("âœ… Connexion ClickHouse vÃ©rifiÃ©e")
    
    # -------------------------------------------------------------------------
    # TEST 4: Insertion directe ClickHouse
    # -------------------------------------------------------------------------
    def test_clickhouse_direct_insert(self, clickhouse_client, sensor_data_factory, unique_rack_id):
        """
        Test: insertion directe dans ClickHouse (sans pipeline).
        VÃ©rifie que ClickHouse est correctement configurÃ©.
        """
        # ARRANGE
        data = sensor_data_factory(rack_id=unique_rack_id, temperature=25.0)
        
        # ACT
        success = insert_directly_to_clickhouse(clickhouse_client, data)
        
        # ASSERT
        assert success, "Insertion directe Ã©chouÃ©e"
        
        # VÃ©rifier que les donnÃ©es sont prÃ©sentes
        count = count_sensor_records(clickhouse_client, unique_rack_id)
        assert count >= 1, f"DonnÃ©es non trouvÃ©es (count={count})"
        
        logger.info(f"âœ… Insertion directe rÃ©ussie pour {unique_rack_id}")
    
    # -------------------------------------------------------------------------
    # TEST 5: Lecture des donnÃ©es ClickHouse
    # -------------------------------------------------------------------------
    def test_clickhouse_read_sensor_data(self, clickhouse_client, sensor_data_factory, unique_rack_id):
        """
        Test: lecture des donnÃ©es de capteurs depuis ClickHouse.
        """
        # ARRANGE - InsÃ©rer des donnÃ©es de test
        data = sensor_data_factory(
            rack_id=unique_rack_id,
            temperature=26.5,
            humidity=72.0,
            co2=900
        )
        insert_directly_to_clickhouse(clickhouse_client, data)
        
        # ACT
        results = query_sensor_data(clickhouse_client, unique_rack_id)
        
        # ASSERT
        assert len(results) >= 1, "Aucune donnÃ©e retournÃ©e"
        
        # VÃ©rifier les valeurs
        row = results[0]
        assert row[0] == unique_rack_id, f"rack_id incorrect: {row[0]}"
        assert abs(row[1] - 26.5) < 0.1, f"TempÃ©rature incorrecte: {row[1]}"
        assert abs(row[2] - 72.0) < 0.1, f"HumiditÃ© incorrecte: {row[2]}"
        assert row[3] == 900, f"CO2 incorrect: {row[3]}"
        
        logger.info(f"âœ… Lecture ClickHouse vÃ©rifiÃ©e: temp={row[1]}Â°C, humidity={row[2]}%")
    
    # -------------------------------------------------------------------------
    # TEST 6: Pipeline MQTT â†’ ClickHouse (End-to-End)
    # -------------------------------------------------------------------------
    @pytest.mark.slow
    def test_mqtt_to_clickhouse_e2e(self, mqtt_client, clickhouse_client, 
                                     sensor_data_factory, unique_rack_id):
        """
        Test E2E: donnÃ©es MQTT arrivent dans ClickHouse.
        
        NOTE:
            Ce test nÃ©cessite que le bridge MQTT-Kafka et le
            stream processor soient actifs. Si ce n'est pas le cas,
            le test sera marquÃ© comme "skipped" aprÃ¨s timeout.
        """
        # ARRANGE
        data = sensor_data_factory(
            rack_id=unique_rack_id,
            temperature=27.3,
            humidity=65.5
        )
        topic = MQTT_TOPIC_SENSORS.format(rack_id=unique_rack_id)
        
        # Compter les enregistrements avant
        initial_count = count_sensor_records(clickhouse_client, unique_rack_id)
        
        # ACT - Publier sur MQTT
        payload = json.dumps(data)
        result = mqtt_client.publish(topic, payload, qos=1)
        result.wait_for_publish(timeout=5)
        
        assert result.is_published(), "Publication MQTT Ã©chouÃ©e"
        logger.info(f"ğŸ“¤ Message publiÃ© sur MQTT: {topic}")
        
        # ASSERT - Attendre que les donnÃ©es arrivent dans ClickHouse
        def check_data_arrived():
            current_count = count_sensor_records(clickhouse_client, unique_rack_id)
            return current_count > initial_count
        
        data_arrived = wait_for_condition(
            check_data_arrived,
            timeout=MESSAGE_PROPAGATION_TIMEOUT,
            interval=POLL_INTERVAL,
            description=f"donnÃ©es dans ClickHouse pour {unique_rack_id}"
        )
        
        if not data_arrived:
            pytest.skip(
                "Pipeline MQTTâ†’Kafkaâ†’ClickHouse non actif. "
                "VÃ©rifiez que NiFi/Kafka Connect et le stream processor sont dÃ©marrÃ©s."
            )
        
        # VÃ©rifier les valeurs
        results = query_sensor_data(clickhouse_client, unique_rack_id)
        assert len(results) >= 1, "DonnÃ©es non trouvÃ©es aprÃ¨s propagation"
        
        row = results[0]
        assert abs(row[1] - 27.3) < 0.1, f"TempÃ©rature incorrecte: {row[1]}"
        
        logger.info(f"âœ… Pipeline E2E vÃ©rifiÃ©: MQTT â†’ ClickHouse en < {MESSAGE_PROPAGATION_TIMEOUT}s")
    
    # -------------------------------------------------------------------------
    # TEST 7: Kafka â†’ ClickHouse (bypass MQTT)
    # -------------------------------------------------------------------------
    @pytest.mark.slow
    def test_kafka_to_clickhouse(self, kafka_producer, clickhouse_client,
                                  sensor_data_factory, unique_rack_id):
        """
        Test: donnÃ©es Kafka arrivent dans ClickHouse.
        
        Teste la partie Kafka â†’ Stream Processor â†’ ClickHouse
        sans passer par MQTT.
        """
        # ARRANGE
        data = sensor_data_factory(
            rack_id=unique_rack_id,
            temperature=28.1,
            co2=920
        )
        
        initial_count = count_sensor_records(clickhouse_client, unique_rack_id)
        
        # ACT - Envoyer sur Kafka
        payload = json.dumps(data).encode('utf-8')
        future = kafka_producer.send(KAFKA_TOPIC_SENSOR_DATA, value=payload)
        kafka_producer.flush()
        
        # Attendre la confirmation d'envoi
        record_metadata = future.get(timeout=10)
        logger.info(
            f"ğŸ“¤ Message Kafka envoyÃ©: topic={record_metadata.topic}, "
            f"partition={record_metadata.partition}, offset={record_metadata.offset}"
        )
        
        # ASSERT - Attendre les donnÃ©es dans ClickHouse
        def check_kafka_data():
            return count_sensor_records(clickhouse_client, unique_rack_id) > initial_count
        
        data_arrived = wait_for_condition(
            check_kafka_data,
            timeout=MESSAGE_PROPAGATION_TIMEOUT,
            interval=POLL_INTERVAL,
            description=f"donnÃ©es Kafka dans ClickHouse pour {unique_rack_id}"
        )
        
        if not data_arrived:
            pytest.skip(
                "Stream Processor non actif. "
                "VÃ©rifiez que le consumer Kafka â†’ ClickHouse est dÃ©marrÃ©."
            )
        
        logger.info("âœ… Pipeline Kafka â†’ ClickHouse vÃ©rifiÃ©")
    
    # -------------------------------------------------------------------------
    # TEST 8: Multiple messages batch
    # -------------------------------------------------------------------------
    @pytest.mark.slow
    def test_batch_messages(self, clickhouse_client, sensor_data_factory, unique_rack_id):
        """
        Test: insertion de plusieurs messages en batch.
        """
        # ARRANGE
        batch_size = 10
        messages = []
        
        for i in range(batch_size):
            data = sensor_data_factory(
                rack_id=unique_rack_id,
                temperature=20.0 + i,  # TempÃ©ratures diffÃ©rentes
                humidity=60.0 + i
            )
            messages.append(data)
        
        # ACT - InsÃ©rer tous les messages
        for msg in messages:
            insert_directly_to_clickhouse(clickhouse_client, msg)
        
        # ASSERT
        count = count_sensor_records(clickhouse_client, unique_rack_id)
        assert count >= batch_size, f"Attendu {batch_size} messages, trouvÃ© {count}"
        
        logger.info(f"âœ… Batch de {batch_size} messages insÃ©rÃ©")
    
    # -------------------------------------------------------------------------
    # TEST 9: DonnÃ©es avec valeurs extrÃªmes
    # -------------------------------------------------------------------------
    def test_extreme_values(self, clickhouse_client, sensor_data_factory, unique_rack_id):
        """
        Test: les valeurs extrÃªmes sont correctement stockÃ©es.
        """
        # ARRANGE - Valeurs aux limites
        data = sensor_data_factory(
            rack_id=unique_rack_id,
            temperature=45.0,   # TrÃ¨s chaud
            humidity=99.9,      # TrÃ¨s humide
            co2=5000,          # CO2 trÃ¨s Ã©levÃ©
            light=2000,        # LumiÃ¨re intense
            ec=4.0,            # EC Ã©levÃ©e
            ph=4.5             # pH acide
        )
        
        # ACT
        success = insert_directly_to_clickhouse(clickhouse_client, data)
        
        # ASSERT
        assert success, "Insertion valeurs extrÃªmes Ã©chouÃ©e"
        
        results = query_sensor_data(clickhouse_client, unique_rack_id)
        assert len(results) >= 1, "DonnÃ©es extrÃªmes non trouvÃ©es"
        
        row = results[0]
        assert row[1] == 45.0, f"TempÃ©rature extrÃªme non stockÃ©e: {row[1]}"
        assert row[3] == 5000, f"CO2 extrÃªme non stockÃ©: {row[3]}"
        
        logger.info("âœ… Valeurs extrÃªmes correctement stockÃ©es")
    
    # -------------------------------------------------------------------------
    # TEST 10: Latence du pipeline
    # -------------------------------------------------------------------------
    @pytest.mark.slow
    def test_pipeline_latency(self, clickhouse_client, sensor_data_factory, unique_rack_id):
        """
        Test: mesure de la latence d'insertion.
        
        OBJECTIF:
            Latence < 100ms pour insertion directe
        """
        import time
        
        # ARRANGE
        data = sensor_data_factory(rack_id=unique_rack_id)
        
        # ACT
        start_time = time.perf_counter()
        success = insert_directly_to_clickhouse(clickhouse_client, data)
        latency_ms = (time.perf_counter() - start_time) * 1000
        
        # ASSERT
        assert success, "Insertion Ã©chouÃ©e"
        assert latency_ms < 500, f"Latence trop Ã©levÃ©e: {latency_ms:.1f}ms"
        
        logger.info(f"âœ… Latence d'insertion: {latency_ms:.1f}ms")


# =============================================================================
# TESTS DE ROBUSTESSE
# =============================================================================

class TestPipelineRobustness:
    """
    Tests de robustesse du pipeline.
    """
    
    # -------------------------------------------------------------------------
    # TEST 1: Messages malformÃ©s
    # -------------------------------------------------------------------------
    def test_malformed_message_handling(self, mqtt_client, unique_rack_id):
        """
        Test: les messages malformÃ©s ne crashent pas le pipeline.
        """
        # ARRANGE - Message JSON invalide
        topic = MQTT_TOPIC_SENSORS.format(rack_id=unique_rack_id)
        invalid_payload = b"not a valid json {"
        
        # ACT
        result = mqtt_client.publish(topic, invalid_payload, qos=1)
        result.wait_for_publish(timeout=5)
        
        # ASSERT - Le message est publiÃ© (MQTT ne valide pas le contenu)
        assert result.is_published(), "Publication Ã©chouÃ©e"
        
        # Le pipeline devrait gÃ©rer ce message sans crash
        logger.info("âœ… Message malformÃ© publiÃ© (handling par le pipeline)")
    
    # -------------------------------------------------------------------------
    # TEST 2: Messages avec champs manquants
    # -------------------------------------------------------------------------
    def test_missing_fields(self, mqtt_client, unique_rack_id):
        """
        Test: messages avec champs manquants sont gÃ©rÃ©s.
        """
        # ARRANGE - Message incomplet
        topic = MQTT_TOPIC_SENSORS.format(rack_id=unique_rack_id)
        incomplete_data = {
            "rack_id": unique_rack_id,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            # Manque "sensors"
        }
        
        # ACT
        result = mqtt_client.publish(topic, json.dumps(incomplete_data), qos=1)
        result.wait_for_publish(timeout=5)
        
        # ASSERT
        assert result.is_published(), "Publication Ã©chouÃ©e"
        logger.info("âœ… Message incomplet publiÃ©")
    
    # -------------------------------------------------------------------------
    # TEST 3: Reconnexion aprÃ¨s dÃ©connexion
    # -------------------------------------------------------------------------
    def test_mqtt_reconnection(self, check_services):
        """
        Test: le client MQTT peut se reconnecter.
        """
        # ARRANGE
        client = get_mqtt_client()
        client.loop_start()
        
        # ACT - DÃ©connecter puis reconnecter
        client.disconnect()
        time.sleep(1)
        
        # Reconnecter
        client.reconnect()
        time.sleep(1)
        
        # ASSERT
        assert client.is_connected(), "Reconnexion Ã©chouÃ©e"
        
        client.loop_stop()
        client.disconnect()
        
        logger.info("âœ… Reconnexion MQTT vÃ©rifiÃ©e")


# =============================================================================
# FIN DU MODULE - TICKET-110 - Tests Integration MQTTâ†’ClickHouse VertiFlow
# =============================================================================
