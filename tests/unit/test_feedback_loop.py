#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
PROJET VERTIFLOW - Agriculture Verticale Intelligente
================================================================================
Date de cr√©ation    : 02/01/2026
√âquipe              : VertiFlow Core Team

Membres de l'√©quipe :
    üßô‚Äç‚ôÇÔ∏è Mounir      - Architecte & Scientifique (Python Dev)
    üèóÔ∏è Imrane      - DevOps & Infrastructure (Python Dev)
    üêç Mouhammed   - Data Engineer & Analyste ETL
    üß¨ Asama       - Biologiste & Domain Expert (Python Dev)
    ‚öñÔ∏è MrZakaria   - Encadrant & Architecte Data

--------------------------------------------------------------------------------
MODULE: tests/unit/test_feedback_loop.py
DESCRIPTION: Tests unitaires pour la boucle de r√©troaction (feedback_loop.py)

    Le Feedback Loop est le syst√®me de R√âAPPRENTISSAGE de VertiFlow.
    Il compare les pr√©dictions pass√©es aux r√©sultats r√©els pour am√©liorer
    continuellement la pr√©cision des mod√®les ML.

    CYCLE DE R√âTROACTION:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                     FEEDBACK LOOP - AM√âLIORATION CONTINUE               ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ                                                                         ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                   ‚îÇ
    ‚îÇ  ‚îÇ  Oracle (J-30)   ‚îÇ‚îÄ‚îÄ‚îê                                                ‚îÇ
    ‚îÇ  ‚îÇ  Pr√©diction:     ‚îÇ  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
    ‚îÇ  ‚îÇ  Yield = 45g     ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  COMPARAISON PR√âDICTION vs R√âALIT√â  ‚îÇ     ‚îÇ
    ‚îÇ  ‚îÇ  Conf: 0.85      ‚îÇ  ‚îÇ    ‚îÇ                                     ‚îÇ     ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ    ‚îÇ  Pr√©diction: 45g ¬± 5g               ‚îÇ     ‚îÇ
    ‚îÇ                        ‚îÇ    ‚îÇ  R√©alit√©:    42g (ClickHouse)       ‚îÇ     ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ    ‚îÇ  Erreur:     3g (6.7%)              ‚îÇ     ‚îÇ
    ‚îÇ  ‚îÇ  ClickHouse      ‚îÇ‚îÄ‚îÄ‚îò    ‚îÇ  Statut:     ‚úÖ DANS TOL√âRANCE      ‚îÇ     ‚îÇ
    ‚îÇ  ‚îÇ  (Aujourd'hui)   ‚îÇ       ‚îÇ                                     ‚îÇ     ‚îÇ
    ‚îÇ  ‚îÇ  R√©colte: 42g    ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ                          ‚îÇ
    ‚îÇ                                              ‚ñº                          ‚îÇ
    ‚îÇ                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
    ‚îÇ                         ‚îÇ        ANALYSE DES √âCARTS               ‚îÇ     ‚îÇ
    ‚îÇ                         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îÇ
    ‚îÇ                         ‚îÇ Si erreur > 15%:                        ‚îÇ     ‚îÇ
    ‚îÇ                         ‚îÇ   ‚Üí D√©clencher r√©entra√Ænement           ‚îÇ     ‚îÇ
    ‚îÇ                         ‚îÇ   ‚Üí Ajuster hyperparam√®tres             ‚îÇ     ‚îÇ
    ‚îÇ                         ‚îÇ   ‚Üí Alerter √©quipe Data Science         ‚îÇ     ‚îÇ
    ‚îÇ                         ‚îÇ                                         ‚îÇ     ‚îÇ
    ‚îÇ                         ‚îÇ Si erreur < 15%:                        ‚îÇ     ‚îÇ
    ‚îÇ                         ‚îÇ   ‚Üí Logger pour analyse tendance        ‚îÇ     ‚îÇ
    ‚îÇ                         ‚îÇ   ‚Üí Accumuler donn√©es entra√Ænement      ‚îÇ     ‚îÇ
    ‚îÇ                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
    ‚îÇ                                          ‚îÇ                              ‚îÇ
    ‚îÇ                                          ‚ñº                              ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
    ‚îÇ  ‚îÇ                    STOCKAGE M√âTRIQUES (MongoDB)                   ‚îÇ  ‚îÇ
    ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îÇ
    ‚îÇ  ‚îÇ  {                                                                ‚îÇ  ‚îÇ
    ‚îÇ  ‚îÇ    "prediction_id": "PRED_20251203_R01_001",                      ‚îÇ  ‚îÇ
    ‚îÇ  ‚îÇ    "predicted_value": 45.0,                                       ‚îÇ  ‚îÇ
    ‚îÇ  ‚îÇ    "actual_value": 42.0,                                          ‚îÇ  ‚îÇ
    ‚îÇ  ‚îÇ    "error_pct": 6.67,                                             ‚îÇ  ‚îÇ
    ‚îÇ  ‚îÇ    "mae": 3.0,                                                    ‚îÇ  ‚îÇ
    ‚îÇ  ‚îÇ    "within_tolerance": true,                                      ‚îÇ  ‚îÇ
    ‚îÇ  ‚îÇ    "evaluated_at": "2026-01-02T12:00:00Z"                         ‚îÇ  ‚îÇ
    ‚îÇ  ‚îÇ  }                                                                ‚îÇ  ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
    ‚îÇ                                                                         ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    M√âTRIQUES DE PERFORMANCE CALCUL√âES:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ M√©trique           ‚îÇ Description                                         ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ MAE                ‚îÇ Mean Absolute Error - Erreur absolue moyenne        ‚îÇ
    ‚îÇ RMSE               ‚îÇ Root Mean Square Error - Sensible aux gros √©carts   ‚îÇ
    ‚îÇ MAPE               ‚îÇ Mean Absolute Percentage Error - Erreur relative    ‚îÇ
    ‚îÇ R¬≤                 ‚îÇ Coefficient de d√©termination - Qualit√© ajustement   ‚îÇ
    ‚îÇ Accuracy@15%       ‚îÇ % pr√©dictions dans ¬±15% de la r√©alit√©               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

IMPORTANCE CRITIQUE:
    Sans feedback loop, les mod√®les ML d√©rivent avec le temps (concept drift).
    Les conditions changent (saisons, nouvelles vari√©t√©s, usure √©quipements)
    et les pr√©dictions deviennent obsol√®tes.

D√©velopp√© par       : @Mounir & @Mouhammed
Ticket(s) associ√©(s): TICKET-107
Sprint              : Semaine 6 - Phase Qualit√© & Tests

D√©pendances:
    - pytest>=8.0.0
    - numpy>=2.1.0
    - pandas>=2.2.0

================================================================================
¬© 2026 VertiFlow Core Team - Tous droits r√©serv√©s
D√©velopp√© dans le cadre de l'Initiative Nationale Marocaine JobInTech
au sein de l'√âcole YNOV Maroc Campus
================================================================================
"""

import os
import pytest
import numpy as np
from datetime import datetime, timedelta, timezone
from unittest.mock import MagicMock, patch, AsyncMock

# =============================================================================
# IMPORT DU MODULE √Ä TESTER
# =============================================================================

try:
    from cloud_citadel.connectors.feedback_loop import FeedbackLoop
    FEEDBACK_LOOP_AVAILABLE = True
except ImportError as e:
    FEEDBACK_LOOP_AVAILABLE = False
    IMPORT_ERROR = str(e)

# Skip tous les tests si le module n'est pas importable
pytestmark = pytest.mark.skipif(
    not FEEDBACK_LOOP_AVAILABLE,
    reason=f"Module feedback_loop non disponible: {IMPORT_ERROR if not FEEDBACK_LOOP_AVAILABLE else ''}"
)


# =============================================================================
# CONSTANTES DE TEST
# =============================================================================

# Seuils de tol√©rance pour les pr√©dictions
TOLERANCE_YIELD_PCT = 15.0      # ¬±15% pour le rendement
TOLERANCE_QUALITY_PCT = 10.0    # ¬±10% pour la qualit√©
TOLERANCE_GROWTH_DAYS = 2       # ¬±2 jours pour la dur√©e de croissance

# Seuils pour d√©clencher le r√©entra√Ænement
RETRAIN_THRESHOLD_MAE = 5.0     # MAE > 5g ‚Üí r√©entra√Ænement
RETRAIN_THRESHOLD_MAPE = 20.0   # MAPE > 20% ‚Üí r√©entra√Ænement
RETRAIN_MIN_SAMPLES = 100       # Minimum d'√©chantillons pour √©valuer

# Identifiants de test
TEST_PREDICTION_ID = "PRED_TEST_20260102_001"
TEST_RACK_ID = "R01"
TEST_CYCLE_ID = "CYCLE_2026_001"


# =============================================================================
# FIXTURES SP√âCIFIQUES AU FEEDBACK LOOP
# =============================================================================

@pytest.fixture
def mock_clickhouse_predictions():
    """
    Mock du client ClickHouse pour r√©cup√©rer les pr√©dictions historiques.
    
    Simule la table `predictions_yield` qui stocke les pr√©dictions
    faites par l'Oracle √† J-30.
    
    COLONNES:
        - prediction_id: Identifiant unique
        - rack_id: Rack concern√©
        - predicted_at: Date de la pr√©diction
        - predicted_yield_g: Rendement pr√©dit (grammes)
        - confidence: Confiance du mod√®le (0-1)
        - target_harvest_date: Date de r√©colte pr√©vue
    """
    mock_client = MagicMock()
    
    # Pr√©dictions historiques (faites il y a 30 jours)
    prediction_date = datetime.now(timezone.utc) - timedelta(days=30)
    harvest_date = datetime.now(timezone.utc)
    
    mock_client.execute.return_value = [
        # (prediction_id, rack_id, predicted_at, predicted_yield_g, confidence, target_date)
        ("PRED_001", "R01", prediction_date, 45.0, 0.85, harvest_date),
        ("PRED_002", "R01", prediction_date, 42.0, 0.82, harvest_date),
        ("PRED_003", "R02", prediction_date, 48.0, 0.88, harvest_date),
        ("PRED_004", "R02", prediction_date, 44.0, 0.79, harvest_date),
        ("PRED_005", "R03", prediction_date, 50.0, 0.91, harvest_date),
    ]
    
    return mock_client


@pytest.fixture
def mock_clickhouse_actuals():
    """
    Mock du client ClickHouse pour r√©cup√©rer les valeurs r√©elles (r√©coltes).
    
    Simule la table `harvest_records` qui enregistre les r√©coltes effectives.
    
    COLONNES:
        - harvest_id: Identifiant unique
        - rack_id: Rack r√©colt√©
        - harvested_at: Date de r√©colte
        - actual_yield_g: Rendement r√©el (grammes)
        - quality_score: Score qualit√© (0-100)
    """
    mock_client = MagicMock()
    
    # R√©coltes effectu√©es (aujourd'hui)
    harvest_date = datetime.now(timezone.utc)
    
    mock_client.execute.return_value = [
        # (harvest_id, rack_id, harvested_at, actual_yield_g, quality_score)
        ("HARV_001", "R01", harvest_date, 42.0, 88),  # -3g vs pr√©dit
        ("HARV_002", "R01", harvest_date, 40.0, 85),  # -2g vs pr√©dit
        ("HARV_003", "R02", harvest_date, 52.0, 92),  # +4g vs pr√©dit
        ("HARV_004", "R02", harvest_date, 43.0, 82),  # -1g vs pr√©dit
        ("HARV_005", "R03", harvest_date, 47.0, 90),  # -3g vs pr√©dit
    ]
    
    return mock_client


@pytest.fixture
def mock_mongodb_metrics():
    """
    Mock du client MongoDB pour stocker les m√©triques de feedback.
    
    Collection: `feedback_metrics`
    Stocke l'historique des comparaisons pr√©diction vs r√©alit√©.
    """
    mock_client = MagicMock()
    mock_db = MagicMock()
    mock_collection = MagicMock()
    
    mock_client.__getitem__ = MagicMock(return_value=mock_db)
    mock_db.__getitem__ = MagicMock(return_value=mock_collection)
    mock_db.feedback_metrics = mock_collection
    
    # insert_one retourne un r√©sultat avec inserted_id
    mock_collection.insert_one.return_value = MagicMock(
        inserted_id="60f1234567890abcdef12345"
    )
    
    # find retourne un curseur mockable
    mock_collection.find.return_value = []
    
    return mock_client


@pytest.fixture
def feedback_instance(mock_clickhouse_predictions, mock_clickhouse_actuals, mock_mongodb_metrics):
    """
    Cr√©e une instance de FeedbackLoop avec d√©pendances mock√©es.
    """
    with patch('cloud_citadel.connectors.feedback_loop.Client') as mock_ch_class:
        # Le m√™me mock pour les deux usages (pr√©dictions et actuals)
        mock_ch_class.return_value = mock_clickhouse_predictions
        
        with patch('cloud_citadel.connectors.feedback_loop.MongoClient') as mock_mongo_class:
            mock_mongo_class.return_value = mock_mongodb_metrics
            
            # Cr√©er l'instance
            loop = FeedbackLoop()
            
            # Injecter les mocks
            loop.ch_client = mock_clickhouse_predictions
            loop.mongo_client = mock_mongodb_metrics
            loop.db_ml = mock_mongodb_metrics['vertiflow_ml']
            
            yield loop


@pytest.fixture
def sample_predictions():
    """
    √âchantillon de pr√©dictions pour les tests de calcul.
    
    Format: Liste de dicts avec predicted et actual.
    """
    return [
        {"predicted": 45.0, "actual": 42.0},  # -6.67%
        {"predicted": 42.0, "actual": 40.0},  # -4.76%
        {"predicted": 48.0, "actual": 52.0},  # +8.33%
        {"predicted": 44.0, "actual": 43.0},  # -2.27%
        {"predicted": 50.0, "actual": 47.0},  # -6.00%
    ]


@pytest.fixture
def sample_predictions_with_drift():
    """
    √âchantillon avec drift significatif (erreur √©lev√©e).
    
    Simule un mod√®le qui n√©cessite un r√©entra√Ænement.
    """
    return [
        {"predicted": 45.0, "actual": 32.0},  # -28.9%
        {"predicted": 42.0, "actual": 30.0},  # -28.6%
        {"predicted": 48.0, "actual": 35.0},  # -27.1%
        {"predicted": 44.0, "actual": 31.0},  # -29.5%
        {"predicted": 50.0, "actual": 36.0},  # -28.0%
    ]


# =============================================================================
# CLASSE DE TEST: CALCUL DES M√âTRIQUES D'ERREUR
# =============================================================================

class TestErrorMetricsCalculation:
    """
    Tests du calcul des m√©triques d'erreur de pr√©diction.
    
    Ces m√©triques quantifient la pr√©cision des mod√®les ML.
    """
    
    # -------------------------------------------------------------------------
    # TEST 1: Calcul du MAE (Mean Absolute Error)
    # -------------------------------------------------------------------------
    def test_calculate_mae(self, feedback_instance, sample_predictions):
        """
        Test: calcul correct du MAE.
        
        MAE = (1/n) √ó Œ£|predicted - actual|
        
        DONN√âES:
            |45-42| + |42-40| + |48-52| + |44-43| + |50-47|
            = 3 + 2 + 4 + 1 + 3 = 13
            MAE = 13 / 5 = 2.6g
        """
        # ARRANGE
        predictions = [p["predicted"] for p in sample_predictions]
        actuals = [p["actual"] for p in sample_predictions]
        
        # EXPECTED
        expected_mae = 2.6
        
        # ACT
        mae = feedback_instance.calculate_mae(predictions, actuals)
        
        # ASSERT
        assert abs(mae - expected_mae) < 0.01, (
            f"MAE incorrect.\n"
            f"Attendu: {expected_mae}g\n"
            f"Obtenu: {mae:.2f}g"
        )
    
    # -------------------------------------------------------------------------
    # TEST 2: Calcul du RMSE (Root Mean Square Error)
    # -------------------------------------------------------------------------
    def test_calculate_rmse(self, feedback_instance, sample_predictions):
        """
        Test: calcul correct du RMSE.
        
        RMSE = ‚àö[(1/n) √ó Œ£(predicted - actual)¬≤]
        
        DONN√âES:
            (3¬≤ + 2¬≤ + 4¬≤ + 1¬≤ + 3¬≤) = 9 + 4 + 16 + 1 + 9 = 39
            RMSE = ‚àö(39/5) = ‚àö7.8 ‚âà 2.79g
        """
        # ARRANGE
        predictions = [p["predicted"] for p in sample_predictions]
        actuals = [p["actual"] for p in sample_predictions]
        
        # EXPECTED
        expected_rmse = np.sqrt(39 / 5)  # ‚âà 2.79
        
        # ACT
        rmse = feedback_instance.calculate_rmse(predictions, actuals)
        
        # ASSERT
        assert abs(rmse - expected_rmse) < 0.01, (
            f"RMSE incorrect.\n"
            f"Attendu: {expected_rmse:.2f}g\n"
            f"Obtenu: {rmse:.2f}g"
        )
    
    # -------------------------------------------------------------------------
    # TEST 3: Calcul du MAPE (Mean Absolute Percentage Error)
    # -------------------------------------------------------------------------
    def test_calculate_mape(self, feedback_instance, sample_predictions):
        """
        Test: calcul correct du MAPE.
        
        MAPE = (100/n) √ó Œ£|predicted - actual| / actual
        
        DONN√âES:
            3/42 + 2/40 + 4/52 + 1/43 + 3/47
            = 0.0714 + 0.05 + 0.0769 + 0.0233 + 0.0638
            = 0.2854
            MAPE = 28.54 / 5 = 5.71%
        """
        # ARRANGE
        predictions = [p["predicted"] for p in sample_predictions]
        actuals = [p["actual"] for p in sample_predictions]
        
        # EXPECTED (calcul manuel)
        errors = [abs(p - a) / a for p, a in zip(predictions, actuals)]
        expected_mape = np.mean(errors) * 100
        
        # ACT
        mape = feedback_instance.calculate_mape(predictions, actuals)
        
        # ASSERT
        assert abs(mape - expected_mape) < 0.1, (
            f"MAPE incorrect.\n"
            f"Attendu: {expected_mape:.2f}%\n"
            f"Obtenu: {mape:.2f}%"
        )
    
    # -------------------------------------------------------------------------
    # TEST 4: Calcul du R¬≤ (coefficient de d√©termination)
    # -------------------------------------------------------------------------
    def test_calculate_r_squared(self, feedback_instance, sample_predictions):
        """
        Test: calcul correct du R¬≤.
        
        R¬≤ = 1 - (SS_res / SS_tot)
        
        O√π:
            SS_res = Œ£(actual - predicted)¬≤
            SS_tot = Œ£(actual - mean(actual))¬≤
        
        R¬≤ proche de 1 = bonnes pr√©dictions
        R¬≤ proche de 0 = pr√©dictions pas meilleures que la moyenne
        R¬≤ < 0 = pr√©dictions pires que la moyenne
        """
        # ARRANGE
        predictions = [p["predicted"] for p in sample_predictions]
        actuals = [p["actual"] for p in sample_predictions]
        
        # EXPECTED
        mean_actual = np.mean(actuals)
        ss_res = sum((a - p) ** 2 for p, a in zip(predictions, actuals))
        ss_tot = sum((a - mean_actual) ** 2 for a in actuals)
        expected_r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
        
        # ACT
        r2 = feedback_instance.calculate_r_squared(predictions, actuals)
        
        # ASSERT
        assert abs(r2 - expected_r2) < 0.01, (
            f"R¬≤ incorrect.\n"
            f"Attendu: {expected_r2:.4f}\n"
            f"Obtenu: {r2:.4f}"
        )
    
    # -------------------------------------------------------------------------
    # TEST 5: Accuracy dans la tol√©rance
    # -------------------------------------------------------------------------
    def test_calculate_accuracy_within_tolerance(self, feedback_instance, sample_predictions):
        """
        Test: calcul du % de pr√©dictions dans la tol√©rance.
        
        TOL√âRANCE: ¬±15%
        
        DONN√âES:
            45 vs 42: erreur 7.1% ‚Üí ‚úÖ dans tol√©rance
            42 vs 40: erreur 5.0% ‚Üí ‚úÖ dans tol√©rance
            48 vs 52: erreur 7.7% ‚Üí ‚úÖ dans tol√©rance
            44 vs 43: erreur 2.3% ‚Üí ‚úÖ dans tol√©rance
            50 vs 47: erreur 6.4% ‚Üí ‚úÖ dans tol√©rance
        
        Accuracy = 5/5 = 100%
        """
        # ARRANGE
        predictions = [p["predicted"] for p in sample_predictions]
        actuals = [p["actual"] for p in sample_predictions]
        
        # ACT
        accuracy = feedback_instance.calculate_accuracy_at_tolerance(
            predictions, actuals, tolerance_pct=15.0
        )
        
        # ASSERT
        assert accuracy == 100.0, (
            f"Toutes les pr√©dictions sont dans ¬±15%, accuracy devrait √™tre 100%.\n"
            f"Obtenu: {accuracy:.1f}%"
        )
    
    # -------------------------------------------------------------------------
    # TEST 6: Accuracy avec drift
    # -------------------------------------------------------------------------
    def test_calculate_accuracy_with_drift(self, feedback_instance, sample_predictions_with_drift):
        """
        Test: accuracy faible quand le mod√®le d√©rive.
        
        DONN√âES (drift ~28%):
            Toutes les erreurs > 15% ‚Üí aucune dans la tol√©rance
            Accuracy = 0%
        """
        # ARRANGE
        predictions = [p["predicted"] for p in sample_predictions_with_drift]
        actuals = [p["actual"] for p in sample_predictions_with_drift]
        
        # ACT
        accuracy = feedback_instance.calculate_accuracy_at_tolerance(
            predictions, actuals, tolerance_pct=15.0
        )
        
        # ASSERT
        assert accuracy == 0.0, (
            f"Aucune pr√©diction n'est dans ¬±15% (drift ~28%), accuracy devrait √™tre 0%.\n"
            f"Obtenu: {accuracy:.1f}%"
        )


# =============================================================================
# CLASSE DE TEST: COMPARAISON PR√âDICTION VS R√âALIT√â
# =============================================================================

class TestPredictionComparison:
    """
    Tests de la comparaison entre pr√©dictions et valeurs r√©elles.
    """
    
    # -------------------------------------------------------------------------
    # TEST 1: Comparaison simple
    # -------------------------------------------------------------------------
    def test_compare_single_prediction(self, feedback_instance):
        """
        Test: comparaison d'une pr√©diction unique avec la r√©alit√©.
        
        ENTR√âE:
            Prediction: 45.0g
            Actual: 42.0g
        
        SORTIE ATTENDUE:
            {
                "error_absolute": 3.0,
                "error_pct": 7.14,
                "within_tolerance": True
            }
        """
        # ACT
        result = feedback_instance.compare_prediction(
            predicted=45.0,
            actual=42.0,
            tolerance_pct=15.0
        )
        
        # ASSERT
        assert result is not None, "Le r√©sultat ne devrait pas √™tre None"
        
        assert "error_absolute" in result or "absolute_error" in result, (
            "Erreur absolue manquante"
        )
        
        error_abs = result.get("error_absolute", result.get("absolute_error"))
        assert abs(error_abs - 3.0) < 0.01, f"Erreur absolue incorrecte: {error_abs}"
        
        assert "within_tolerance" in result, "Statut tol√©rance manquant"
        assert result["within_tolerance"] is True, "Devrait √™tre dans la tol√©rance"
    
    # -------------------------------------------------------------------------
    # TEST 2: Comparaison hors tol√©rance
    # -------------------------------------------------------------------------
    def test_compare_prediction_out_of_tolerance(self, feedback_instance):
        """
        Test: d√©tection d'une pr√©diction hors tol√©rance.
        
        ENTR√âE:
            Prediction: 45.0g
            Actual: 32.0g
            Erreur: 40.6% > 15%
        """
        # ACT
        result = feedback_instance.compare_prediction(
            predicted=45.0,
            actual=32.0,
            tolerance_pct=15.0
        )
        
        # ASSERT
        assert result["within_tolerance"] is False, (
            f"Erreur de 40% devrait √™tre hors tol√©rance.\n"
            f"R√©sultat: {result}"
        )
    
    # -------------------------------------------------------------------------
    # TEST 3: Comparaison batch
    # -------------------------------------------------------------------------
    def test_compare_batch_predictions(self, feedback_instance, sample_predictions):
        """
        Test: comparaison d'un lot de pr√©dictions.
        
        ENTR√âE:
            Liste de 5 paires (predicted, actual)
        
        SORTIE:
            Liste de 5 r√©sultats de comparaison
        """
        # ARRANGE
        predictions = [p["predicted"] for p in sample_predictions]
        actuals = [p["actual"] for p in sample_predictions]
        
        # ACT
        results = feedback_instance.compare_batch(predictions, actuals)
        
        # ASSERT
        assert len(results) == len(sample_predictions), (
            f"Nombre de r√©sultats incorrect.\n"
            f"Attendu: {len(sample_predictions)}\n"
            f"Obtenu: {len(results)}"
        )


# =============================================================================
# CLASSE DE TEST: D√âTECTION DE DRIFT
# =============================================================================

class TestDriftDetection:
    """
    Tests de la d√©tection de d√©rive du mod√®le (concept drift).
    
    CONCEPT DRIFT:
        Ph√©nom√®ne o√π la relation entre les features et la target
        change au fil du temps, rendant le mod√®le obsol√®te.
    
    CAUSES POSSIBLES:
        - Changement de vari√©t√© cultiv√©e
        - Saisonnalit√© (√©t√© vs hiver)
        - Usure des √©quipements (LED, capteurs)
        - Nouvelles pratiques de culture
    """
    
    # -------------------------------------------------------------------------
    # TEST 1: Pas de drift d√©tect√©
    # -------------------------------------------------------------------------
    def test_no_drift_detected(self, feedback_instance, sample_predictions):
        """
        Test: pas de drift quand les erreurs sont faibles.
        
        CRIT√àRES DE DRIFT:
            - MAE > 5.0g ‚Üí drift
            - MAPE > 20% ‚Üí drift
        
        DONN√âES: MAE ‚âà 2.6g, MAPE ‚âà 5.7% ‚Üí pas de drift
        """
        # ARRANGE
        predictions = [p["predicted"] for p in sample_predictions]
        actuals = [p["actual"] for p in sample_predictions]
        
        # ACT
        drift_detected = feedback_instance.detect_drift(predictions, actuals)
        
        # ASSERT
        assert drift_detected is False, (
            f"Pas de drift attendu (MAE ~2.6g, MAPE ~5.7%).\n"
            f"Drift d√©tect√©: {drift_detected}"
        )
    
    # -------------------------------------------------------------------------
    # TEST 2: Drift d√©tect√©
    # -------------------------------------------------------------------------
    def test_drift_detected(self, feedback_instance, sample_predictions_with_drift):
        """
        Test: drift d√©tect√© quand les erreurs sont √©lev√©es.
        
        DONN√âES: MAE ~13g, MAPE ~28% ‚Üí drift d√©tect√©
        """
        # ARRANGE
        predictions = [p["predicted"] for p in sample_predictions_with_drift]
        actuals = [p["actual"] for p in sample_predictions_with_drift]
        
        # ACT
        drift_detected = feedback_instance.detect_drift(predictions, actuals)
        
        # ASSERT
        assert drift_detected is True, (
            f"Drift attendu (MAPE ~28%).\n"
            f"Drift d√©tect√©: {drift_detected}"
        )
    
    # -------------------------------------------------------------------------
    # TEST 3: Seuils de drift configurables
    # -------------------------------------------------------------------------
    def test_drift_threshold_configurable(self, feedback_instance, sample_predictions):
        """
        Test: les seuils de drift peuvent √™tre configur√©s.
        
        Avec un seuil tr√®s bas, m√™me des erreurs faibles
        devraient d√©clencher une d√©tection de drift.
        """
        # ARRANGE
        predictions = [p["predicted"] for p in sample_predictions]
        actuals = [p["actual"] for p in sample_predictions]
        
        # ACT - Seuil tr√®s bas (MAE < 1g)
        drift_low_threshold = feedback_instance.detect_drift(
            predictions, actuals, mae_threshold=1.0
        )
        
        # ACT - Seuil normal
        drift_normal_threshold = feedback_instance.detect_drift(
            predictions, actuals, mae_threshold=5.0
        )
        
        # ASSERT
        # Avec seuil bas, drift devrait √™tre d√©tect√©
        # Avec seuil normal, pas de drift
        assert drift_low_threshold is True or drift_normal_threshold is False, (
            "Les seuils devraient affecter la d√©tection de drift"
        )


# =============================================================================
# CLASSE DE TEST: D√âCLENCHEMENT DU R√âENTRA√éNEMENT
# =============================================================================

class TestRetrainingTrigger:
    """
    Tests du d√©clenchement automatique du r√©entra√Ænement.
    
    PROCESSUS:
        1. Drift d√©tect√©
        2. V√©rifier que suffisamment de donn√©es sont disponibles
        3. Cr√©er une t√¢che de r√©entra√Ænement
        4. Notifier l'√©quipe Data Science
    """
    
    # -------------------------------------------------------------------------
    # TEST 1: R√©entra√Ænement d√©clench√© sur drift
    # -------------------------------------------------------------------------
    def test_trigger_retraining_on_drift(self, feedback_instance, sample_predictions_with_drift):
        """
        Test: le r√©entra√Ænement est d√©clench√© quand un drift est d√©tect√©.
        """
        # ARRANGE
        predictions = [p["predicted"] for p in sample_predictions_with_drift]
        actuals = [p["actual"] for p in sample_predictions_with_drift]
        
        # ACT
        should_retrain = feedback_instance.should_trigger_retraining(
            predictions, actuals
        )
        
        # ASSERT
        assert should_retrain is True, (
            "Le r√©entra√Ænement devrait √™tre d√©clench√© sur drift"
        )
    
    # -------------------------------------------------------------------------
    # TEST 2: Pas de r√©entra√Ænement si pas de drift
    # -------------------------------------------------------------------------
    def test_no_retraining_without_drift(self, feedback_instance, sample_predictions):
        """
        Test: pas de r√©entra√Ænement si les performances sont bonnes.
        """
        # ARRANGE
        predictions = [p["predicted"] for p in sample_predictions]
        actuals = [p["actual"] for p in sample_predictions]
        
        # ACT
        should_retrain = feedback_instance.should_trigger_retraining(
            predictions, actuals
        )
        
        # ASSERT
        assert should_retrain is False, (
            "Le r√©entra√Ænement ne devrait pas √™tre d√©clench√© sans drift"
        )
    
    # -------------------------------------------------------------------------
    # TEST 3: Minimum de donn√©es requis
    # -------------------------------------------------------------------------
    def test_retraining_requires_minimum_samples(self, feedback_instance):
        """
        Test: le r√©entra√Ænement n√©cessite un minimum de donn√©es.
        
        RAISON:
            √âviter les faux positifs sur un √©chantillon trop petit.
            Minimum: 100 comparaisons.
        """
        # ARRANGE - Seulement 3 √©chantillons (< 100 requis)
        predictions = [45.0, 30.0, 25.0]  # Erreurs √©normes
        actuals = [10.0, 10.0, 10.0]
        
        # ACT
        should_retrain = feedback_instance.should_trigger_retraining(
            predictions, actuals, min_samples=100
        )
        
        # ASSERT
        # M√™me avec de grosses erreurs, pas de r√©entra√Ænement si < 100 samples
        assert should_retrain is False, (
            "Le r√©entra√Ænement ne devrait pas √™tre d√©clench√© avec < 100 √©chantillons"
        )


# =============================================================================
# CLASSE DE TEST: STOCKAGE DES M√âTRIQUES
# =============================================================================

class TestMetricsStorage:
    """
    Tests du stockage des m√©triques de feedback dans MongoDB.
    """
    
    # -------------------------------------------------------------------------
    # TEST 1: Stockage r√©ussi
    # -------------------------------------------------------------------------
    def test_store_feedback_metrics(self, feedback_instance, mock_mongodb_metrics, sample_predictions):
        """
        Test: les m√©triques sont correctement stock√©es dans MongoDB.
        """
        # ARRANGE
        predictions = [p["predicted"] for p in sample_predictions]
        actuals = [p["actual"] for p in sample_predictions]
        
        metrics = {
            "mae": 2.6,
            "rmse": 2.79,
            "mape": 5.71,
            "r_squared": 0.85,
            "accuracy_at_15pct": 100.0,
            "evaluated_at": datetime.now(timezone.utc),
            "sample_count": len(sample_predictions)
        }
        
        # ACT
        result = feedback_instance.store_metrics(metrics)
        
        # ASSERT
        assert result is not None, "Le stockage devrait retourner un r√©sultat"
        mock_mongodb_metrics['vertiflow_ml'].feedback_metrics.insert_one.assert_called()
    
    # -------------------------------------------------------------------------
    # TEST 2: M√©triques incluent le timestamp
    # -------------------------------------------------------------------------
    def test_stored_metrics_have_timestamp(self, feedback_instance, mock_mongodb_metrics):
        """
        Test: les m√©triques stock√©es incluent un timestamp.
        """
        # ARRANGE
        metrics = {"mae": 2.5, "rmse": 3.0}
        
        # ACT
        feedback_instance.store_metrics(metrics)
        
        # ASSERT - V√©rifier que le timestamp a √©t√© ajout√©
        call_args = mock_mongodb_metrics['vertiflow_ml'].feedback_metrics.insert_one.call_args
        if call_args:
            stored_doc = call_args[0][0] if call_args[0] else {}
            # Le timestamp devrait √™tre pr√©sent
            assert "evaluated_at" in stored_doc or "timestamp" in stored_doc or "created_at" in stored_doc, (
                "Timestamp manquant dans le document stock√©"
            )


# =============================================================================
# CLASSE DE TEST: CYCLE COMPLET DE FEEDBACK
# =============================================================================

class TestFeedbackCycle:
    """
    Tests du cycle complet de feedback (run_cycle).
    
    CYCLE:
        1. R√©cup√©rer les pr√©dictions √† √©valuer
        2. R√©cup√©rer les valeurs r√©elles correspondantes
        3. Calculer les m√©triques
        4. D√©tecter le drift
        5. Stocker les r√©sultats
        6. D√©clencher le r√©entra√Ænement si n√©cessaire
    """
    
    # -------------------------------------------------------------------------
    # TEST 1: Cycle complet sans drift
    # -------------------------------------------------------------------------
    def test_full_cycle_no_drift(self, feedback_instance):
        """
        Test: cycle complet quand le mod√®le performe bien.
        """
        # ACT
        result = feedback_instance.run_cycle()
        
        # ASSERT
        assert result is not None, "Le cycle devrait retourner un r√©sultat"
        
        # V√©rifier les composants du r√©sultat
        if isinstance(result, dict):
            assert "metrics" in result or "mae" in result, "M√©triques manquantes"
            assert "drift_detected" in result or "drift" in result, "Statut drift manquant"
    
    # -------------------------------------------------------------------------
    # TEST 2: Cycle g√©n√®re des m√©triques valides
    # -------------------------------------------------------------------------
    def test_cycle_generates_valid_metrics(self, feedback_instance):
        """
        Test: les m√©triques g√©n√©r√©es sont valides et compl√®tes.
        """
        # ACT
        result = feedback_instance.run_cycle()
        
        # ASSERT
        if result and isinstance(result, dict):
            metrics = result.get("metrics", result)
            
            # MAE devrait √™tre >= 0
            if "mae" in metrics:
                assert metrics["mae"] >= 0, "MAE devrait √™tre >= 0"
            
            # MAPE devrait √™tre >= 0
            if "mape" in metrics:
                assert metrics["mape"] >= 0, "MAPE devrait √™tre >= 0"
            
            # R¬≤ devrait √™tre <= 1
            if "r_squared" in metrics:
                assert metrics["r_squared"] <= 1, "R¬≤ devrait √™tre <= 1"


# =============================================================================
# CLASSE DE TEST: CAS LIMITES
# =============================================================================

class TestEdgeCases:
    """
    Tests des cas limites du feedback loop.
    """
    
    # -------------------------------------------------------------------------
    # TEST 1: Listes vides
    # -------------------------------------------------------------------------
    def test_empty_predictions_list(self, feedback_instance):
        """
        Test: gestion des listes vides.
        """
        # ACT & ASSERT
        try:
            mae = feedback_instance.calculate_mae([], [])
            # Si pas d'exception, v√©rifier que le r√©sultat est g√©r√©
            assert mae == 0 or mae is None or np.isnan(mae), (
                "MAE sur liste vide devrait √™tre 0, None ou NaN"
            )
        except (ValueError, ZeroDivisionError):
            pass  # Exception acceptable pour liste vide
    
    # -------------------------------------------------------------------------
    # TEST 2: Valeur r√©elle √† z√©ro
    # -------------------------------------------------------------------------
    def test_actual_value_zero(self, feedback_instance):
        """
        Test: gestion des valeurs r√©elles √† z√©ro (√©viter division par z√©ro).
        
        Le MAPE divise par la valeur r√©elle, donc actual=0 est probl√©matique.
        """
        # ARRANGE
        predictions = [10.0, 20.0, 30.0]
        actuals = [0.0, 15.0, 25.0]  # Premier √©l√©ment = 0
        
        # ACT & ASSERT
        try:
            mape = feedback_instance.calculate_mape(predictions, actuals)
            # Ne devrait pas √™tre infini
            assert not np.isinf(mape), "MAPE ne devrait pas √™tre infini"
        except (ValueError, ZeroDivisionError):
            pass  # Exception acceptable
    
    # -------------------------------------------------------------------------
    # TEST 3: Listes de tailles diff√©rentes
    # -------------------------------------------------------------------------
    def test_mismatched_list_sizes(self, feedback_instance):
        """
        Test: gestion des listes de tailles diff√©rentes.
        """
        # ARRANGE
        predictions = [10.0, 20.0, 30.0]
        actuals = [15.0, 25.0]  # Taille diff√©rente
        
        # ACT & ASSERT
        try:
            feedback_instance.calculate_mae(predictions, actuals)
            # Si pas d'exception, v√©rifier le comportement
        except (ValueError, IndexError):
            pass  # Exception attendue pour tailles diff√©rentes


# =============================================================================
# FIN DU MODULE - TICKET-107 - Tests Feedback Loop VertiFlow
# =============================================================================
