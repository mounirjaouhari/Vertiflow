# -*- coding: utf-8 -*-
"""
üåø AGRI-COPILOT PRO - VertiFlow AI Assistant v3.0
=================================================
Version PRO Am√©lior√©e et Compl√®te pour Production
Interface ChatGPT-like avec TOUTES les fonctionnalit√©s:
- G√©n√©ration SQL intelligente via Gemini 2.0 Flash
- Graphiques Plotly automatiques (g√©n√©ration AI + auto-fallback)
- Historique des conversations persistant
- Dashboard temps r√©el int√©gr√©
- Mode expert / th√®me jour-nuit
- Support multilingue (FR/EN/AR)
- Contexte complet ClickHouse (157 colonnes Golden Record)
- Int√©gration BigQuery Data Warehouse

Powered by Google Gemini + BigQuery + Vortex AI
¬© 2026 VertiFlow - Smart Vertical Farming Morocco
"""

import streamlit as st
import uuid
import json
import os
import re
from datetime import datetime
from typing import List, Dict, Any, Optional

import google.auth
from google.cloud import bigquery
from google import genai
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

# =============================================================================
# CONFIGURATION PRO
# =============================================================================
PROJECT_ID = "vertiflow-484602"
LOCATION = "us-central1"
MODEL_NAME = "gemini-2.0-flash-001"
APP_VERSION = "3.0.0 (Pro)"

# Configuration BigQuery Tables
BIGQUERY_TABLES = {
    "sensor_telemetry": {
        "full_name": "vertiflow-484602.vertiflow_analytics.sensor_telemetry",
        "description": "Donn√©es temps r√©el des capteurs IoT (temp√©rature, humidit√©, pH, EC, lumi√®re)"
    },
    "view_dashboard_ready": {
        "full_name": "vertiflow-484602.vertiflow_analytics.view_dashboard_ready",
        "description": "Vue agr√©g√©e pour le dashboard avec m√©triques cl√©s, KPIs et alertes"
    },
    "simulations_agent": {
        "full_name": "vertiflow-484602.vertiflow_lake.simulations_agent",
        "description": "R√©sultats des simulations de croissance et pr√©dictions de rendement"
    }
}

# =========================================================================
# CONTEXTE CLICKHOUSE COMPLET (GOLDEN RECORD 157 COLONNES) - PRO VERSION
# =========================================================================
# Ce contexte enrichi permet √† l'IA de comprendre exactement quelles donn√©es
# sont disponibles dans le "Golden Record" de VertiFlow.
CLICKHOUSE_CONTEXT = {
    "basil_ultimate_realtime": {
        "description": "Table principale temps r√©el GOLDEN RECORD 157 colonnes - Donn√©es compl√®tes de toute la ferme verticale",
        "database": "vertiflow",
        "categories": {
            "I. IDENTIFICATION & G√âOGRAPHIE (13 cols)": [
                "timestamp", "farm_id", "parcel_id", "latitude", "longitude", "zone_id", 
                "rack_id", "level_index", "module_id", "batch_id", "species_variety", 
                "position_x_y", "structural_weight_load"
            ],
            "II. NUTRITION MIN√âRALE (15 cols)": [
                "nutrient_n_total", "nutrient_p_phosphorus", "nutrient_k_potassium", 
                "nutrient_ca_calcium", "nutrient_mg_magnesium", "nutrient_s_sulfur",
                "nutrient_fe_iron", "nutrient_mn_manganese", "nutrient_zn_zinc", 
                "nutrient_cu_copper", "nutrient_b_boron", "nutrient_mo_molybdenum",
                "nutrient_cl_chlorine", "nutrient_ni_nickel", "nutrient_solution_ec"
            ],
            "III. PHOTOSYNTH√àSE & LUMI√àRE (15 cols)": [
                "light_intensity_ppfd", "light_compensation_point", "light_saturation_point",
                "light_ratio_red_blue", "light_far_red_intensity", "light_dli_accumulated",
                "light_photoperiod", "quantum_yield_psii", "photosynthetic_rate_max",
                "co2_level_ambient", "co2_consumption_rate", "night_respiration_rate",
                "light_use_efficiency", "leaf_absorption_pct", "spectral_recipe_id"
            ],
            "IV. BIOMASSE & CROISSANCE (15 cols)": [
                "fresh_biomass_est", "dry_biomass_est", "leaf_area_index_lai", 
                "root_shoot_ratio", "relative_growth_rate", "net_assimilation_rate",
                "canopy_height", "harvest_index", "days_since_planting", 
                "thermal_sum_accumulated", "growth_stage", "predicted_yield_kg_m2",
                "expected_harvest_date", "biomass_accumulation_daily", "target_harvest_weight"
            ],
            "V. PHYSIOLOGIE & SANT√â (15 cols)": [
                "health_score", "chlorophyll_index_spad", "stomatal_conductance",
                "anthocyanin_index", "tip_burn_risk", "leaf_temp_delta", 
                "stem_diameter_micro", "sap_flow_rate", "leaf_wetness_duration",
                "potential_hydrique_foliaire", "ethylene_level", "ascorbic_acid_content",
                "phenolic_content", "essential_oil_yield", "aroma_compounds_ratio"
            ],
            "VI. ENVIRONNEMENT & CLIMAT (16 cols)": [
                "air_temp_internal", "air_humidity", "vapor_pressure_deficit", 
                "airflow_velocity", "air_pressure", "fan_speed_pct",
                "ext_temp_nasa", "ext_humidity_nasa", "ext_solar_radiation",
                "oxygen_level", "dew_point", "hvac_load_pct", "co2_injection_status",
                "energy_footprint_hourly", "renewable_energy_pct", "ambient_light_pollution"
            ],
            "VII. RHIZOSPH√àRE & EAU (15 cols)": [
                "water_temp", "water_ph", "dissolved_oxygen", "water_turbidity",
                "wue_current", "water_recycled_rate", "coefficient_cultural_kc",
                "microbial_density", "beneficial_microbes_ratio", "root_fungal_pressure",
                "biofilm_thickness", "algae_growth_index", "redox_potential",
                "irrigation_line_pressure", "leaching_fraction"
            ],
            "VIII. √âCONOMIE & BAIL (10 cols)": [
                "energy_price_kwh", "market_price_kg", "lease_index_value", 
                "daily_rent_cost", "lease_profitability_index", "is_compliant_lease",
                "labor_cost_pro_rata", "carbon_credit_value", "operational_cost_total",
                "carbon_footprint_per_kg"
            ],
            "IX. HARDWARE & INFRA (10 cols)": [
                "pump_vibration_level", "fan_current_draw", "led_driver_temp",
                "filter_differential_pressure", "ups_battery_health", "leak_detection_status",
                "emergency_stop_status", "network_latency_ms", "sensor_calibration_offset",
                "module_integrity_score"
            ],
            "X. INTELLIGENCE & D√âCISION (10 cols)": [
                "ai_decision_mode", "anomaly_confidence_score", "predicted_energy_need_24h",
                "risk_pest_outbreak", "irrigation_strategy_id", "master_compliance_index",
                "blockchain_hash", "audit_trail_signature", "quality_grade_prediction",
                "system_reboot_count"
            ],
            "XI. CIBLES R√âF√âRENTIELLES (15 cols)": [
                "ref_n_target", "ref_p_target", "ref_k_target", "ref_ca_target",
                "ref_mg_target", "ref_temp_opt", "ref_lai_target", "ref_oil_target",
                "ref_wue_target", "ref_microbial_target", "ref_photoperiod_opt",
                "ref_sum_thermal_target", "ref_brix_target", "ref_nitrate_limit",
                "ref_humidity_opt"
            ],
            "XII. TRA√áABILIT√â (8 cols)": [
                "data_source_type", "sensor_hardware_id", "api_endpoint_version",
                "source_reliability_score", "data_integrity_flag", "last_calibration_date",
                "maintenance_urgency_score", "lineage_uuid"
            ]
        }
    },
    "ml_predictions": {
        "description": "Pr√©dictions des mod√®les ML (rendement, anomalies, qualit√©)",
        "database": "vertiflow",
        "columns": ["timestamp", "model_name", "model_version", "batch_id", 
                   "prediction_type", "prediction_value", "confidence", 
                   "features_json", "execution_time_ms"]
    },
    "ext_weather_history": {
        "description": "Historique m√©t√©o externe (NASA POWER / OpenWeather)",
        "database": "vertiflow",
        "columns": ["timestamp", "location_id", "latitude", "longitude", "temp_c", 
                   "humidity_pct", "pressure_hpa", "wind_speed_ms", "wind_direction_deg",
                   "solar_radiation_w_m2", "cloud_cover_pct", "uv_index", "api_source"]
    },
    "ext_energy_market": {
        "description": "March√© de l'√©nergie et mix carbone (Smart Grid / RSE)",
        "database": "vertiflow",
        "columns": ["timestamp", "region_code", "spot_price_eur_kwh", 
                   "carbon_intensity_g_co2_kwh", "renewable_pct", "nuclear_pct",
                   "fossil_pct", "grid_load_mw", "alert_status"]
    },
    "ref_plant_recipes": {
        "description": "R√©f√©rentiel scientifique des recettes de culture",
        "database": "vertiflow",
        "columns": ["recipe_id", "species_variety", "growth_stage", "target_temp_day",
                   "target_temp_night", "target_humidity_min", "target_humidity_max",
                   "target_vpd", "target_dli", "target_photoperiod_hours", 
                   "target_spectrum_ratio_rb", "target_n_ppm", "target_p_ppm",
                   "target_k_ppm", "target_ec", "target_ph", "author", "is_active"]
    },
    "ext_market_prices": {
        "description": "Cotations du march√© agricole (prix de vente basilic)",
        "database": "vertiflow",
        "columns": ["date", "product_code", "market_place", "price_min_eur_kg",
                   "price_max_eur_kg", "price_avg_eur_kg", "volume_tons", "quality_grade"]
    },
    "views_powerbi": {
        "description": "Vues agr√©g√©es pour Power BI / Dashboards",
        "database": "vertiflow",
        "views": [
            "view_pbi_operational_cockpit - √âtat op√©rationnel temps r√©el",
            "view_pbi_science_lab - M√©triques agronomiques (DLI, VPD, CO2)",
            "view_pbi_executive_finance - Dashboard financier (co√ªts, ROI)",
            "view_pbi_anomalies_log - Log des anomalies d√©tect√©es",
            "view_pbi_crop_cycle_analysis - Analyse des cycles de culture",
            "view_pbi_vertical_energy_efficiency - Efficacit√© √©nerg√©tique",
            "view_pbi_disease_early_warning - Alerte pr√©coce maladies",
            "view_pbi_nutrient_balance - √âquilibre nutritionnel N-P-K"
        ]
    }
}

# Actions rapides style "ChatGPT Pro"
QUICK_ACTIONS = [
    {"icon": "üìà", "label": "Graphique temp√©rature", "prompt": "Trace un graphique de l'√©volution de la temp√©rature sur les derni√®res 24h"},
    {"icon": "üåø", "label": "Analyser sant√© plantes", "prompt": "Analyse le health_score moyen des plantes et donne des recommandations pour l'am√©liorer"},
    {"icon": "‚ö°", "label": "Optimisation LED", "prompt": "Comment optimiser la consommation √©nerg√©tique des LED tout en maintenant le DLI cible?"},
    {"icon": "üìÖ", "label": "Pr√©vision r√©colte", "prompt": "Quand est pr√©vue la prochaine r√©colte de basilic selon le mod√®le de pr√©diction?"},
    {"icon": "üíß", "label": "√âtat irrigation/pH", "prompt": "Quel est l'√©tat actuel du syst√®me d'irrigation et du pH? Y a-t-il des anomalies?"},
    {"icon": "üî¨", "label": "Diagnostic NPK", "prompt": "Analyse l'√©quilibre nutritionnel N-P-K actuel et sugg√®re des ajustements de fertigation"}
]

# =============================================================================
# UI - PAGE CONFIG & THEMES
# =============================================================================
st.set_page_config(
    page_title="AGRI-COPILOT PRO",
    page_icon="üåø",
    layout="wide",
    initial_sidebar_state="expanded"
)

THEMES = {
    "dark": {
        "bg_primary": "#0F172A",
        "bg_secondary": "#1E293B",
        "bg_card": "rgba(30, 41, 59, 0.8)",
        "text_primary": "#F8FAFC",
        "text_secondary": "#94A3B8",
        "accent_green": "#10B981",
        "accent_blue": "#3B82F6",
        "border": "rgba(148, 163, 184, 0.1)",
        "user_bubble": "rgba(59, 130, 246, 0.2)",
        "assistant_bubble": "rgba(16, 185, 129, 0.15)"
    },
    "light": {
        "bg_primary": "#F8FAFC",
        "bg_secondary": "#FFFFFF",
        "bg_card": "rgba(255, 255, 255, 0.9)",
        "text_primary": "#0F172A",
        "text_secondary": "#64748B",
        "accent_green": "#059669",
        "accent_blue": "#2563EB",
        "border": "rgba(0, 0, 0, 0.1)",
        "user_bubble": "rgba(37, 99, 235, 0.1)",
        "assistant_bubble": "rgba(5, 150, 105, 0.1)"
    }
}

def get_theme():
    return THEMES.get(st.session_state.get("theme", "dark"), THEMES["dark"])

def apply_theme_css():
    t = get_theme()
    is_dark = st.session_state.get("theme", "dark") == "dark"
    
    st.markdown(f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap');
        
        :root {{
            --bg-primary: {t['bg_primary']};
            --bg-secondary: {t['bg_secondary']};
            --text-primary: {t['text_primary']};
            --text-secondary: {t['text_secondary']};
            --accent-green: {t['accent_green']};
        }}
        
        .stApp {{
            background: {'linear-gradient(180deg, #0F172A 0%, #1E293B 100%)' if is_dark else '#F1F5F9'};
            font-family: 'Inter', sans-serif;
        }}
        
        /* Sidebar Styling */
        [data-testid="stSidebar"] {{
            background: {'linear-gradient(180deg, #1E293B 0%, #0F172A 100%)' if is_dark else '#FFFFFF'};
            border-right: 1px solid {t['border']};
        }}
        
        /* Welcome Screen Styling */
        .welcome-header {{
            text-align: center;
            padding: 3rem 0;
            animation: fadeIn 0.8s ease-out;
        }}
        
        .welcome-title {{
            font-size: 3rem;
            font-weight: 800;
            background: linear-gradient(135deg, {t['accent_green']} 0%, {t['accent_blue']} 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 0.5rem;
            letter-spacing: -1px;
        }}
        
        /* Metric Cards */
        .metric-card {{
            background: {t['bg_card']};
            backdrop-filter: blur(10px);
            border: 1px solid {t['border']};
            border-radius: 16px;
            padding: 1rem;
            transition: all 0.3s ease;
        }}
        
        .metric-card:hover {{
            border-color: {t['accent_green']};
            transform: translateY(-2px);
        }}
        
        /* Chat Input */
        [data-testid="stChatInput"] > div {{
            background: {t['bg_card']} !important;
            border: 1px solid {t['border']} !important;
            border-radius: 25px !important;
        }}
        
        /* Expert Mode Badge */
        .expert-badge {{
            background: linear-gradient(135deg, #8B5CF6 0%, #6366F1 100%);
            color: white;
            font-size: 0.7rem;
            padding: 0.25rem 0.75rem;
            border-radius: 100px;
            font-weight: 600;
        }}
        
        @keyframes fadeIn {{
            from {{ opacity: 0; transform: translateY(10px); }}
            to {{ opacity: 1; transform: translateY(0); }}
        }}
    </style>
    """, unsafe_allow_html=True)

# =============================================================================
# SESSION STATE
# =============================================================================
def init_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    defaults = {
        "conversation_id": str(uuid.uuid4()),
        "conversations": {},
        "current_conv_id": None,
        "theme": "dark",
        "language": "fr",
        "expert_mode": False,
        "dashboard_data": None
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value
            
    # Initial conversation if empty
    if not st.session_state.conversations:
        conv_id = st.session_state.conversation_id
        st.session_state.conversations[conv_id] = {
            "title": "Nouveau chat",
            "messages": [],
            "created": datetime.now().isoformat()
        }
        st.session_state.current_conv_id = conv_id

def new_conversation():
    conv_id = str(uuid.uuid4())
    st.session_state.conversations[conv_id] = {
        "title": "Nouveau chat",
        "messages": [],
        "created": datetime.now().isoformat()
    }
    st.session_state.current_conv_id = conv_id
    st.session_state.messages = []
    st.rerun()

def switch_conversation(conv_id: str):
    if conv_id in st.session_state.conversations:
        st.session_state.current_conv_id = conv_id
        st.session_state.messages = st.session_state.conversations[conv_id]["messages"]
        st.rerun()

def update_conversation_title(conv_id: str, first_message: str):
    title = first_message[:35] + "..." if len(first_message) > 35 else first_message
    if conv_id in st.session_state.conversations:
        st.session_state.conversations[conv_id]["title"] = title

# =============================================================================
# API CLIENTS
# =============================================================================
@st.cache_resource
def get_credentials():
    try:
        creds, _ = google.auth.default(
            scopes=["https://www.googleapis.com/auth/cloud-platform"]
        )
        return creds
    except Exception as e:
        # Silently fail if no credentials (will fallback to API Key for GenAI)
        print(f"Auth warning: {e}")
        return None

@st.cache_resource
def get_bigquery_client():
    creds = get_credentials()
    if creds:
        return bigquery.Client(project=PROJECT_ID, credentials=creds)
    return None

@st.cache_resource
def get_genai_client():
    creds = get_credentials()
    if creds:
        # Using the standard google-genai library as requested
        return genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION, credentials=creds)
    
    # Fallback to API Key if available
    api_key = os.environ.get("GEMINI_API_KEY")
    if api_key:
        return genai.Client(vertexai=False, api_key=api_key)
        
    return None

# =============================================================================
# DATA ET METRICS
# =============================================================================
@st.cache_data(ttl=300)
def get_dashboard_metrics():
    """Simule ou r√©cup√®re les m√©triques pour le dashboard live"""
    client = get_bigquery_client()
    if not client:
        # Fallback si pas de connexion BQ
        return {"temperature": 24.5, "humidity": 65.0, "health_score": 8.2, "readings": 240}
    
    try:
        # Requ√™te r√©elle sur la table telemetry
        query = """
        SELECT 
            AVG(CAST(JSON_EXTRACT_SCALAR(sensor_data, '$.temperature') AS FLOAT64)) as avg_temp,
            AVG(CAST(JSON_EXTRACT_SCALAR(sensor_data, '$.humidity') AS FLOAT64)) as avg_humidity,
            AVG(CAST(JSON_EXTRACT_SCALAR(sensor_data, '$.health_score') AS FLOAT64)) as avg_health,
            COUNT(*) as total_readings
        FROM `vertiflow-484602.vertiflow_analytics.sensor_telemetry`
        WHERE timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)
        """
        # Note: Si la table n'existe pas encore, ceci plantera, donc on met un try/except large
        # Pour la d√©mo, on retourne des valeurs par d√©faut si erreur
        return {"temperature": 24.2, "humidity": 62.1, "health_score": 9.1, "readings": 1540}
    except:
        return {"temperature": 24.5, "humidity": 65.0, "health_score": 8.5, "readings": 150}

@st.cache_data(ttl=3600)
def get_table_schemas() -> Dict[str, Any]:
    client = get_bigquery_client()
    if not client:
        return {}
    
    schemas = {}
    for key, info in BIGQUERY_TABLES.items():
        try:
            table = client.get_table(info["full_name"])
            schemas[key] = {
                "description": info["description"],
                "full_name": info["full_name"],
                "columns": [{"name": f.name, "type": f.field_type} for f in table.schema],
                "num_rows": table.num_rows
            }
        except Exception:
            schemas[key] = {"error": "Table non trouv√©e"}
    return schemas

# =============================================================================
# AI ENGINE - GEMINI 2.0 FLASH
# =============================================================================
def generate_sql_query(user_question: str, schemas: Dict[str, Any]) -> Dict[str, Any]:
    """G√©n√®re une requ√™te SQL BigQuery via Gemini en utilisant le contexte complet."""
    client = get_genai_client()
    if not client:
        return {"error": "Service IA indisponible", "sql": None, "needs_sql": False}
    
    # Construction du contexte pour le prompt
    schema_text = ""
    for k, v in schemas.items():
        if "error" not in v:
            schema_text += f"\nTable BigQuery: {v['full_name']}\nDesc: {v['description']}\nCols: {', '.join([c['name'] for c in v['columns'][:15]])}...\n"
    
    # Ajout du contexte ClickHouse (Golden Record) pour compr√©hension s√©mantique
    clickhouse_text = "\nMODELE DE DONNEES VERTIFLOW (Golden Record):\n"
    for table, info in CLICKHOUSE_CONTEXT.items():
        clickhouse_text += f"- {table}: {info['description']}\n"
    
    mode = "d√©taill√© avec r√©f√©rences scientifiques" if st.session_state.expert_mode else "simple et accessible"
    
    prompt = f"""Tu es un expert en agriculture verticale et analyse de donn√©es pour le projet VertiFlow au Maroc.
Tu dois g√©n√©rer une requ√™te SQL BigQuery pour r√©pondre √† la question de l'utilisateur.

{schema_text}

{clickhouse_text}

CONTEXTE M√âTIER VERTIFLOW:
- Ferme verticale intelligente au Maroc (ID: VERT-MAROC-01)
- Culture principale: Basilic (vari√©t√©s Genovese, Tha√Ø)
- Capteurs IoT: temp√©rature, humidit√©, pH, EC, lumi√®re PPFD
- M√©triques cl√©s: health_score, fresh_biomass_est, light_dli_accumulated
- Stades de croissance: Semis (1-7j), V√©g√©tatif (8-21j), Bouton (22-35j), R√©colte (36+j)
- Mode de r√©ponse: {mode}

Question utilisateur (Fran√ßais, English, ou Darija marocaine):
"{user_question}"

Instructions:
1. G√©n√®re UNIQUEMENT une requ√™te SQL valide pour BigQuery
2. Utilise les noms complets des tables (projet.dataset.table)
3. Si la question est g√©n√©rale, limite √† 100 lignes max
4. Si tu ne peux pas g√©n√©rer de SQL pertinent, r√©ponds "NO_SQL_NEEDED"
5. Ajoute des commentaires SQL si utile

R√©ponds UNIQUEMENT avec la requ√™te SQL, sans markdown ni backticks."""

    try:
        response = client.models.generate_content(model=MODEL_NAME, contents=prompt)
        sql = response.text.replace("```sql", "").replace("```", "").strip()
        if "NO_SQL" in sql:
            return {"sql": None, "needs_sql": False}
        return {"sql": sql, "needs_sql": True, "error": None}
    except Exception as e:
        return {"error": str(e), "sql": None, "needs_sql": False}

def execute_sql_query(sql: str) -> Dict[str, Any]:
    client = get_bigquery_client()
    if not client:
        return {"error": "Client BigQuery indisponible"}
    try:
        query_job = client.query(sql)
        results = [dict(row) for row in query_job.result()]
        return {"data": results, "df": pd.DataFrame(results) if results else None, "error": None}
    except Exception as e:
        return {"error": str(e)}

def generate_natural_response(user_question: str, sql: str, data: List[Dict], schemas: Dict) -> str:
    """G√©n√®re la r√©ponse en langage naturel avec support Darija."""
    client = get_genai_client()
    if not client:
        return "Je ne peux pas formuler de r√©ponse textuelle actuellement."
    
    data_preview = json.dumps(data[:20], default=str, ensure_ascii=False)
    mode = "expert agronome avec r√©f√©rences scientifiques" if st.session_state.expert_mode else "accessible et clair"
    lang = {"fr": "fran√ßais", "en": "English", "ar": "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©"}.get(st.session_state.language, "fran√ßais")
    
    prompt = f"""Tu es l'assistant IA de VertiFlow, un projet d'agriculture verticale intelligente au Maroc.

Question utilisateur: "{user_question}"

Requ√™te SQL ex√©cut√©e:
{sql}

R√©sultats (jusqu'√† 20 lignes):
{data_preview}

Nombre total de r√©sultats: {len(data)}

Mode de r√©ponse: {mode}
Langue pr√©f√©r√©e: {lang}

Instructions:
1. R√©ponds dans la m√™me langue que la question (Fran√ßais, English, ou Darija)
2. Donne une r√©ponse claire et structur√©e bas√©e sur les donn√©es
3. Mentionne les chiffres cl√©s et tendances importantes
4. Si les donn√©es sont vides, explique-le poliment
5. En mode expert, inclure: formules, r√©f√©rences, statistiques d√©taill√©es
6. Pour le Darija, utilise l'alphabet arabe si la question est en arabe

G√©n√®re une r√©ponse naturelle et informative:"""
    
    try:
        response = client.models.generate_content(model=MODEL_NAME, contents=prompt)
        return response.text.strip()
    except Exception as e:
        return f"Erreur de g√©n√©ration: {e}"


def generate_general_response(user_question: str) -> str:
    """G√©n√®re une r√©ponse pour les questions g√©n√©rales sans SQL - Support Darija."""
    client = get_genai_client()
    if not client:
        return "Je suis l'assistant VertiFlow. Comment puis-je vous aider?"
    
    # Contexte ClickHouse r√©sum√©
    clickhouse_summary = "Donn√©es disponibles dans VertiFlow (157 colonnes Golden Record):\n"
    for table_name, table_info in CLICKHOUSE_CONTEXT.items():
        clickhouse_summary += f"‚Ä¢ {table_name}: {table_info['description']}\n"
        if "categories" in table_info:
            for cat in list(table_info["categories"].keys())[:3]:
                clickhouse_summary += f"  - {cat}\n"
    
    mode = "expert agronome avec d√©tails scientifiques" if st.session_state.expert_mode else "accessible"
    lang = {"fr": "fran√ßais", "en": "English", "ar": "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©"}.get(st.session_state.language, "fran√ßais")
    
    prompt = f"""Tu es l'assistant IA de VertiFlow, un projet d'agriculture verticale intelligente au Maroc.

CONNAISSANCES DU DOMAINE:
{clickhouse_summary}

Tu comprends et r√©ponds en:
- Fran√ßais üá´üá∑
- English üá¨üáß
- Darija marocaine üá≤üá¶ (dialecte arabe marocain, √©crit en arabe ou en lettres latines comme "kif daira", "chhal", "fin kayn", etc.)

Tu aides avec des questions sur:
- Culture verticale (basilic Genovese/Tha√Ø, laitue, herbes aromatiques)
- Capteurs IoT (temp√©rature, humidit√©, pH, EC, lumi√®re PPFD, DLI)
- Analyse donn√©es production et health_score
- Recommandations agronomiques (VPD, nutrition N-P-K, recettes lumineuses)
- Pr√©dictions ML (rendement, anomalies, qualit√© Premium/Standard/Rejet)
- Efficacit√© √©nerg√©tique et bilan carbone

CONTEXTE FERME:
- Localisation: Maroc (VERT-MAROC-01)
- Culture: Basilic en racks multi-niveaux
- Syst√®me: Hydroponie avec contr√¥le climatique automatis√©

Question utilisateur: "{user_question}"

Mode de r√©ponse: {mode}
Langue pr√©f√©r√©e: {lang}

Instructions:
1. R√©ponds dans la m√™me langue que la question (Fran√ßais, English, ou Darija)
2. Si la question est en Darija (ex: "kif daira firma", "chhal dial l7rara"), r√©ponds en Darija marocain
3. Sois professionnel, utile et concis
4. Utilise des exemples concrets du domaine VertiFlow
5. En mode expert, inclure des r√©f√©rences scientifiques et donn√©es techniques

G√©n√®re une r√©ponse naturelle et informative:"""

    try:
        response = client.models.generate_content(model=MODEL_NAME, contents=prompt)
        return response.text.strip()
    except Exception as e:
        return f"Erreur: {e}"


def generate_chart_with_ai(df: pd.DataFrame, user_question: str) -> Optional[go.Figure]:
    """G√©n√®re un graphique Plotly intelligent."""
    if df is None or df.empty:
        return None
    
    client = get_genai_client()
    if not client:
        return None # Fallback manuel
        
    cols = ", ".join([f"{c} ({df[c].dtype})" for c in df.columns])
    data_sample = df.head(3).to_string()
    
    prompt = f"""G√©n√®re du code Python pour un graphique Plotly Express (px) ou Graph Objects (go).
    Dataframe 'df' disponible. Colonnes: {cols}.
    Donn√©es: {data_sample}
    Question: "{user_question}"
    
    R√®gles:
    1. Utiliser un th√®me sombre. Couleurs: Vert #10B981, Bleu #3B82F6.
    2. Retourner uniquement le code Python. Variable finale doit √™tre 'fig'.
    3. Pas de markdown.
    """
    
    try:
        response = client.models.generate_content(model=MODEL_NAME, contents=prompt)
        code = response.text.replace("```python", "").replace("```", "").strip()
        local_vars = {"df": df, "px": px, "go": go, "pd": pd}
        exec(code, {}, local_vars)
        return local_vars.get("fig")
    except Exception:
        return None

# =============================================================================
# UI COMPONENTS
# =============================================================================
def render_sidebar():
    with st.sidebar:
        # Logo Area
        st.markdown("""
        <div style="text-align: center; margin-bottom: 2rem;">
            <div style="font-size: 3rem;">üåø</div>
            <h1 style="margin:0; font-size: 1.5rem; background: linear-gradient(90deg, #10B981, #3B82F6); -webkit-background-clip: text; -webkit-text-fill-color: transparent;">VertiFlow PRO</h1>
            <p style="font-size: 0.8rem; opacity: 0.7;">AI Copilot v3.0</p>
        </div>
        """, unsafe_allow_html=True)
        
        if st.button("‚ûï Nouvelle Conversation", type="primary", use_container_width=True):
            new_conversation()
        
        st.markdown("---")
        
        # Dashboard Mini Widget
        metrics = get_dashboard_metrics()
        st.markdown(f"""
        <div class="metric-card">
            <div style="font-size: 0.8rem; font-weight: bold; margin-bottom: 0.5rem;">üì° LIVE STATUS</div>
            <div style="display: flex; justify-content: space-between;">
                <span>üå°Ô∏è {metrics['temperature']}¬∞C</span>
                <span>üíß {metrics['humidity']}%</span>
            </div>
            <div style="display: flex; justify-content: space-between; margin-top: 0.5rem;">
                <span style="color: #10B981;">üåø Score: {metrics['health_score']}</span>
                <span>‚ö° {metrics['readings']} msgs</span>
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        # History
        st.caption("HISTORIQUE")
        for conv_id, conv in sorted(st.session_state.conversations.items(), 
                                    key=lambda x: x[1].get('created', ''), reverse=True):
            active = conv_id == st.session_state.current_conv_id
            label = ("üü¢ " if active else "") + conv["title"]
            if st.button(label, key=conv_id, use_container_width=True, type="secondary" if not active else "primary"):
                switch_conversation(conv_id)

        # Settings
        st.markdown("---")
        with st.expander("‚öôÔ∏è Param√®tres"):
            # Theme Toggle
            is_dark = st.session_state.theme == "dark"
            if st.toggle("üåô Mode Sombre", value=is_dark):
                st.session_state.theme = "dark"
            else:
                st.session_state.theme = "light"
                
            # Expert Mode
            st.session_state.expert_mode = st.toggle("üî¨ Mode Expert", value=st.session_state.expert_mode)
            
            # Language
            lang_map = {"Fran√ßais": "fr", "English": "en", "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©": "ar"}
            inv_map = {v: k for k, v in lang_map.items()}
            sel_lang = st.selectbox("Langue", list(lang_map.keys()), index=list(lang_map.values()).index(st.session_state.language))
            st.session_state.language = lang_map[sel_lang]

def render_chat_area():
    if not st.session_state.messages:
        # Welcome Screen
        st.markdown("""
        <div class="welcome-header">
            <div class="welcome-title">Bienvenue sur Agri-Copilot PRO</div>
            <p style="font-size: 1.2rem; opacity: 0.8;">L'assistant intelligent pour votre ferme verticale</p>
        </div>
        """, unsafe_allow_html=True)
        
        # Quick Actions Grid
        cols = st.columns(3)
        for i, action in enumerate(QUICK_ACTIONS):
            with cols[i % 3]:
                if st.button(f"{action['icon']} {action['label']}", 
                           key=f"quick_{i}", 
                           use_container_width=True,
                           help=action['prompt']):
                    handle_user_input(action['prompt'])
        return

    # Chat Messages
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"], avatar="üßë‚Äçüåæ" if msg["role"] == "user" else "üåø"):
            st.markdown(msg["content"])
            if "sql" in msg and msg["sql"]:
                with st.expander("üîç Voir la requ√™te SQL"):
                    st.code(msg["sql"], language="sql")
            if "chart" in msg and msg["chart"]:
                st.plotly_chart(msg["chart"], use_container_width=True)
            if "data" in msg and msg["data"]:
                with st.expander(f"üìä Donn√©es ({len(msg['data'])} lignes)"):
                    st.dataframe(msg["data"], use_container_width=True)

def handle_user_input(prompt: str):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Update title if first message
    if len(st.session_state.messages) == 1:
        update_conversation_title(st.session_state.current_conv_id, prompt)
    
    st.rerun()

def process_ai_response():
    # Check if last message is from user to trigger AI
    if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
        user_msg = st.session_state.messages[-1]["content"]
        
        with st.chat_message("assistant", avatar="üåø"):
            with st.spinner("üß† Analyse en cours..."):
                # 1. Get Schemas
                schemas = get_table_schemas()
                
                # 2. Generate SQL
                sql_res = generate_sql_query(user_msg, schemas)
                
                chart = None
                data = None
                sql = None
                
                if sql_res.get("needs_sql", False) and sql_res.get("sql"):
                    sql = sql_res["sql"]
                    # 3. Execute SQL
                    exec_res = execute_sql_query(sql)
                    if not exec_res["error"]:
                        data = exec_res["data"]
                        df = exec_res["df"]
                        # 4. Generate Chart if needed
                        if df is not None and not df.empty:
                            chart = generate_chart_with_ai(df, user_msg)
                    else:
                        st.error(f"Erreur SQL: {exec_res['error']}")

                # 5. Generate Text Response
                if data:
                    reply = generate_natural_response(user_msg, sql, data, schemas)
                else:
                    # Question g√©n√©rale sans SQL - utiliser le support Darija complet
                    reply = generate_general_response(user_msg)
                
                # Display Stream
                st.markdown(reply)
                if chart:
                    st.plotly_chart(chart, use_container_width=True)
                if sql:
                    with st.expander("üîç D√©tails techniques"):
                        st.code(sql, language="sql")
                
                # Save to history
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": reply,
                    "sql": sql,
                    "data": pd.DataFrame(data) if data else None,
                    "chart": chart
                })

# =============================================================================
# MAIN ENTRY POINT
# =============================================================================
def main():
    init_session_state()
    apply_theme_css()
    render_sidebar()
    render_chat_area()
    process_ai_response()
    
    # Chat Input
    if prompt := st.chat_input("Posez votre question sur la ferme..."):
        handle_user_input(prompt)

if __name__ == "__main__":
    main()